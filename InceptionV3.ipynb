{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNKzE3ARO+/Pp281xJwFOzk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimjaehwankimjaehwan/python_/blob/main/InceptionV3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFKzUka4aZ62",
        "outputId": "4784c2a7-26d9-4e34-f3ed-6344ca020f8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels.h5\n",
            "\u001b[1m96112376/96112376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 0us/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 12s/step\n",
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/imagenet_class_index.json\n",
            "\u001b[1m35363/35363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1us/step\n",
            "Predicted: [('n03891251', 'park_bench', 0.15180102), ('n03777754', 'modem', 0.049145788), ('n03223299', 'doormat', 0.04304461)]\n",
            "1: park_bench (0.15)\n",
            "2: modem (0.05)\n",
            "3: doormat (0.04)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_sample_image\n",
        "from tensorflow.keras.applications import InceptionV3\n",
        "from tensorflow.keras.applications.inception_v3 import preprocess_input, decode_predictions\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "\n",
        "# 1.  모델 로드\n",
        "model = InceptionV3(weights='imagenet')\n",
        "\n",
        "# 2. sklearn에서 제공하는 샘플 이미지 로드\n",
        "# 사용 가능한 샘플 이미지: 'china.jpg', 'flower.jpg'\n",
        "image = load_sample_image('china.jpg')  # 또는 'flower.jpg'\n",
        "image = img_to_array(image)\n",
        "\n",
        "# 3. 이미지 전처리\n",
        "# InceptionV3 모델에 입력할 수 있도록 이미지 크기를 299x299로 조정하고, 전처리\n",
        "image = np.resize(image, (299, 299, 3))\n",
        "image = np.expand_dims(image, axis=0)\n",
        "image = preprocess_input(image)\n",
        "\n",
        "# 4. 예측 수행\n",
        "predictions = model.predict(image)\n",
        "\n",
        "# 5. 예측 결과 디코딩 및 출력\n",
        "decoded_predictions = decode_predictions(predictions, top=3)[0]\n",
        "print(\"Predicted:\", decoded_predictions)\n",
        "\n",
        "# 출력된 결과를 해석\n",
        "for i, (imagenet_id, label, score) in enumerate(decoded_predictions):\n",
        "    print(f\"{i+1}: {label} ({score:.2f})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. park_bench (0.15):\n",
        "\n",
        "  모델은 이미지가 \"park bench\" (공원 벤치)일 가능성을 약 15.18%로 예측했습니다.\n",
        "  이 예측이 가장 높은 확률로, 모델이 이미지에서 공원 벤치와 관련된 특징을 인식했을 가능성이 큽니다.\n",
        "\n",
        "2. modem (0.05):\n",
        "\n",
        "  모델은 이미지가 \"modem\" (모뎀)일 가능성을 약 4.91%로 예측했습니다.\n",
        "  이는 이미지에서 모뎀과 관련된 특징이 일부 인식되었음을 시사합니다.\n",
        "\n",
        "3. doormat (0.04):\n",
        "\n",
        "  모델은 이미지가 \"doormat\" (현관 매트)일 가능성을 약 4.30%로 예측했습니다.\n",
        "  이미지에서 현관 매트와 유사한 특징이 발견되었을 가능성이 있습니다."
      ],
      "metadata": {
        "id": "pyXkH3fpbqoV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_sample_image\n",
        "from tensorflow.keras.applications import InceptionV3\n",
        "from tensorflow.keras.applications.inception_v3 import preprocess_input, decode_predictions\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, ImageDataGenerator\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1. InceptionV3 모델 로드 및 미세 조정 설정\n",
        "base_model = InceptionV3(weights='imagenet', include_top=False)  # 최상위 레이어 제거\n",
        "\n",
        "# 2. 레이어 추가 및 모델 확장\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)  # 드롭아웃 추가\n",
        "predictions = Dense(1000, activation='softmax')(x)  # ImageNet 데이터셋의 클래스 수\n",
        "\n",
        "# 새 모델 정의\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# 3. 일부 레이어 동결 해제 (Fine-tuning)\n",
        "for layer in base_model.layers[-50:]:  # 마지막 50개 레이어는 학습 가능하도록 설정\n",
        "    layer.trainable = True\n",
        "\n",
        "# 4. 모델 컴파일 (하이퍼파라미터 튜닝)\n",
        "model.compile(optimizer=Adam(learning_rate=0.00001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# 5. 데이터 증강 설정\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest',\n",
        "    validation_split=0.2  # 데이터의 20%를 검증 데이터로 사용\n",
        ")\n",
        "\n",
        "# 6. 샘플 이미지 로드 및 전처리 (여기서는 데이터 증강 예시로 사용)\n",
        "# 실제로는 여러 이미지를 사용해야 하므로, 아래는 데이터 로딩의 예시입니다.\n",
        "image = load_sample_image('china.jpg')  # 예제 이미지\n",
        "image = img_to_array(image)\n",
        "image = np.resize(image, (299, 299, 3))  # InceptionV3 입력 크기에 맞추기\n",
        "image = np.expand_dims(image, axis=0)\n",
        "image = preprocess_input(image)\n",
        "\n",
        "# 7. 가상 데이터를 사용한 데이터 증강\n",
        "# 실제로는 여러 샘플을 로드하고 데이터셋을 구축해야 합니다.\n",
        "train_data, val_data, train_labels, val_labels = train_test_split([image]*100, to_categorical([1]*100, 1000), test_size=0.2)\n",
        "\n",
        "train_data = np.squeeze(train_data, axis=1) # Remove the extra dimension from train_data\n",
        "val_data = np.squeeze(val_data, axis=1) # Remove the extra dimension from val_data\n",
        "\n",
        "train_generator = datagen.flow(\n",
        "    np.array(train_data),\n",
        "    np.array(train_labels),\n",
        "    batch_size=32,\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "val_generator = datagen.flow(\n",
        "    np.array(val_data),\n",
        "    np.array(val_labels),\n",
        "    batch_size=32,\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "# 조기 종료 설정\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# 8. 모델 학습\n",
        "model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=50,\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "# 9. 모델 평가 (검증 데이터에서)\n",
        "val_loss, val_accuracy = model.evaluate(val_generator)\n",
        "print(f\"Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}\")\n",
        "\n",
        "# 10. 예측 수행 (여기서는 샘플 이미지로 예시)\n",
        "y_pred = model.predict(image)\n",
        "decoded_predictions = decode_predictions(y_pred, top=3)[0]\n",
        "print(\"Predicted:\", decoded_predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPkt553icA5b",
        "outputId": "caaf3739-290d-4ec1-efa0-f56827f7b6a1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 12s/step - accuracy: 0.0000e+00 - loss: 6.6315 - val_accuracy: 0.0000e+00 - val_loss: 6.5073\n",
            "Epoch 2/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 312ms/step - accuracy: 0.0000e+00 - loss: 6.5239 - val_accuracy: 0.0000e+00 - val_loss: 6.2298\n",
            "Epoch 3/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 297ms/step - accuracy: 0.0312 - loss: 6.3609 - val_accuracy: 0.0000e+00 - val_loss: 6.2129\n",
            "Epoch 4/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 302ms/step - accuracy: 0.0312 - loss: 6.2156 - val_accuracy: 1.0000 - val_loss: 5.7569\n",
            "Epoch 5/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 301ms/step - accuracy: 0.2396 - loss: 5.9154 - val_accuracy: 1.0000 - val_loss: 5.7286\n",
            "Epoch 6/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 310ms/step - accuracy: 0.4167 - loss: 5.7153 - val_accuracy: 1.0000 - val_loss: 5.4394\n",
            "Epoch 7/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 176ms/step - accuracy: 0.4062 - loss: 5.6884 - val_accuracy: 0.7500 - val_loss: 5.4589\n",
            "Epoch 8/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 296ms/step - accuracy: 0.6354 - loss: 5.4693 - val_accuracy: 1.0000 - val_loss: 5.0862\n",
            "Epoch 9/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 303ms/step - accuracy: 0.8646 - loss: 5.2329 - val_accuracy: 1.0000 - val_loss: 4.7980\n",
            "Epoch 10/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 318ms/step - accuracy: 0.9271 - loss: 5.0858 - val_accuracy: 1.0000 - val_loss: 4.6032\n",
            "Epoch 11/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 299ms/step - accuracy: 0.9479 - loss: 4.9803 - val_accuracy: 1.0000 - val_loss: 4.3676\n",
            "Epoch 12/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 298ms/step - accuracy: 1.0000 - loss: 4.6972 - val_accuracy: 1.0000 - val_loss: 3.9786\n",
            "Epoch 13/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 303ms/step - accuracy: 1.0000 - loss: 4.5702 - val_accuracy: 1.0000 - val_loss: 3.6558\n",
            "Epoch 14/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 176ms/step - accuracy: 1.0000 - loss: 4.4181 - val_accuracy: 1.0000 - val_loss: 3.7837\n",
            "Epoch 15/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 305ms/step - accuracy: 1.0000 - loss: 4.1958 - val_accuracy: 1.0000 - val_loss: 3.5078\n",
            "Epoch 16/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 297ms/step - accuracy: 1.0000 - loss: 4.0954 - val_accuracy: 1.0000 - val_loss: 3.2603\n",
            "Epoch 17/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 315ms/step - accuracy: 1.0000 - loss: 3.8154 - val_accuracy: 1.0000 - val_loss: 2.8616\n",
            "Epoch 18/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 177ms/step - accuracy: 1.0000 - loss: 3.5831 - val_accuracy: 1.0000 - val_loss: 2.9595\n",
            "Epoch 19/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 307ms/step - accuracy: 1.0000 - loss: 3.5354 - val_accuracy: 1.0000 - val_loss: 2.5197\n",
            "Epoch 20/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 300ms/step - accuracy: 1.0000 - loss: 3.2939 - val_accuracy: 1.0000 - val_loss: 2.2887\n",
            "Epoch 21/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 308ms/step - accuracy: 1.0000 - loss: 3.1245 - val_accuracy: 1.0000 - val_loss: 2.2194\n",
            "Epoch 22/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 177ms/step - accuracy: 1.0000 - loss: 2.9451 - val_accuracy: 1.0000 - val_loss: 2.2576\n",
            "Epoch 23/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 306ms/step - accuracy: 1.0000 - loss: 2.7738 - val_accuracy: 1.0000 - val_loss: 1.7251\n",
            "Epoch 24/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 176ms/step - accuracy: 1.0000 - loss: 2.6107 - val_accuracy: 1.0000 - val_loss: 1.8140\n",
            "Epoch 25/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 313ms/step - accuracy: 1.0000 - loss: 2.4130 - val_accuracy: 1.0000 - val_loss: 1.2511\n",
            "Epoch 26/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 308ms/step - accuracy: 1.0000 - loss: 2.1666 - val_accuracy: 1.0000 - val_loss: 1.1077\n",
            "Epoch 27/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 311ms/step - accuracy: 1.0000 - loss: 2.1005 - val_accuracy: 1.0000 - val_loss: 0.8813\n",
            "Epoch 28/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 176ms/step - accuracy: 1.0000 - loss: 1.9602 - val_accuracy: 1.0000 - val_loss: 1.0069\n",
            "Epoch 29/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 309ms/step - accuracy: 1.0000 - loss: 1.7222 - val_accuracy: 1.0000 - val_loss: 0.8812\n",
            "Epoch 30/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 312ms/step - accuracy: 1.0000 - loss: 1.5694 - val_accuracy: 1.0000 - val_loss: 0.7938\n",
            "Epoch 31/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 299ms/step - accuracy: 1.0000 - loss: 1.5211 - val_accuracy: 1.0000 - val_loss: 0.3546\n",
            "Epoch 32/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 173ms/step - accuracy: 1.0000 - loss: 1.3597 - val_accuracy: 1.0000 - val_loss: 0.6795\n",
            "Epoch 33/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 317ms/step - accuracy: 1.0000 - loss: 1.1417 - val_accuracy: 1.0000 - val_loss: 0.3318\n",
            "Epoch 34/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 183ms/step - accuracy: 1.0000 - loss: 1.0886 - val_accuracy: 1.0000 - val_loss: 0.3692\n",
            "Epoch 35/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 314ms/step - accuracy: 1.0000 - loss: 0.9413 - val_accuracy: 1.0000 - val_loss: 0.3048\n",
            "Epoch 36/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 300ms/step - accuracy: 1.0000 - loss: 0.8217 - val_accuracy: 1.0000 - val_loss: 0.1921\n",
            "Epoch 37/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 306ms/step - accuracy: 1.0000 - loss: 0.7533 - val_accuracy: 1.0000 - val_loss: 0.1786\n",
            "Epoch 38/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 318ms/step - accuracy: 1.0000 - loss: 0.6586 - val_accuracy: 1.0000 - val_loss: 0.1701\n",
            "Epoch 39/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 321ms/step - accuracy: 1.0000 - loss: 0.5687 - val_accuracy: 1.0000 - val_loss: 0.1434\n",
            "Epoch 40/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 306ms/step - accuracy: 1.0000 - loss: 0.5359 - val_accuracy: 1.0000 - val_loss: 0.1063\n",
            "Epoch 41/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 178ms/step - accuracy: 1.0000 - loss: 0.4832 - val_accuracy: 1.0000 - val_loss: 0.1128\n",
            "Epoch 42/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 186ms/step - accuracy: 1.0000 - loss: 0.4016 - val_accuracy: 1.0000 - val_loss: 0.1451\n",
            "Epoch 43/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 313ms/step - accuracy: 1.0000 - loss: 0.4048 - val_accuracy: 1.0000 - val_loss: 0.0815\n",
            "Epoch 44/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 309ms/step - accuracy: 1.0000 - loss: 0.3672 - val_accuracy: 1.0000 - val_loss: 0.0440\n",
            "Epoch 45/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 176ms/step - accuracy: 1.0000 - loss: 0.3177 - val_accuracy: 1.0000 - val_loss: 0.0645\n",
            "Epoch 46/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 185ms/step - accuracy: 1.0000 - loss: 0.2724 - val_accuracy: 1.0000 - val_loss: 0.0937\n",
            "Epoch 47/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 178ms/step - accuracy: 1.0000 - loss: 0.2512 - val_accuracy: 1.0000 - val_loss: 0.0638\n",
            "Epoch 48/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 179ms/step - accuracy: 1.0000 - loss: 0.2184 - val_accuracy: 1.0000 - val_loss: 0.0907\n",
            "Epoch 49/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 182ms/step - accuracy: 1.0000 - loss: 0.2365 - val_accuracy: 1.0000 - val_loss: 0.0658\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 1.0000 - loss: 0.0608\n",
            "Validation Loss: 0.06084834784269333, Validation Accuracy: 1.0\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10s/step\n",
            "Predicted: [('n01443537', 'goldfish', 0.73273593), ('n02692877', 'airship', 0.0009896001), ('n02098105', 'soft-coated_wheaten_terrier', 0.0009248814)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델이 성공적으로 학습되었고, 검증 데이터에서 매우 높은 정확도와 낮은 손실 값을 달성한 것을 볼 수 있습니다. 마지막 예측 결과에서 확률이 높은 \"goldfish\" 클래스를 예측한 것을 보면, 모델이 입력 이미지를 잘 학습하고 예측하는 것을 확인할 수 있습니다."
      ],
      "metadata": {
        "id": "XugYhCA3dM5B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "다른 데이터셋을 사용하여 InceptionV3 모델을 평가하겠습니다. 여기서는 CIFAR-10 데이터셋을 사용하여 모델의 성능을 검증하는 방법을 예로 들겠습니다. CIFAR-10은 10개의 클래스(예: 비행기, 자동차, 새, 고양이 등)로 구성된 6만 개의 32x32 크기의 컬러 이미지로 이루어져 있습니다."
      ],
      "metadata": {
        "id": "E7351iAZfuGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.applications import InceptionV3\n",
        "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# 1. CIFAR-10 데이터셋 로드 및 전처리\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# InceptionV3 모델에 맞게 이미지 크기를 75x75로 조정 (InceptionV3의 입력 크기)\n",
        "x_train = tf.image.resize(x_train, (75, 75))\n",
        "x_test = tf.image.resize(x_test, (75, 75))\n",
        "\n",
        "# 데이터 전처리 (픽셀 값 범위 조정 및 전처리 함수 적용)\n",
        "x_train = preprocess_input(x_train)\n",
        "x_test = preprocess_input(x_test)\n",
        "\n",
        "# 레이블을 One-hot 인코딩\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# 2. InceptionV3 모델 로드 및 미세 조정 설정\n",
        "base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(75, 75, 3))\n",
        "\n",
        "# 3. 레이어 추가 및 모델 확장\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "x = BatchNormalization()(x)  # 배치 정규화 추가\n",
        "x = Dropout(0.7)(x)  # 드롭아웃 비율 증가\n",
        "predictions = Dense(10, activation='softmax')(x)  # CIFAR-10 데이터셋의 클래스 수\n",
        "\n",
        "# 모델 정의\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# 4. 일부 레이어 동결 해제 (Fine-tuning)\n",
        "for layer in base_model.layers[-50:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "# 5. 모델 컴파일 (학습률 조정)\n",
        "model.compile(optimizer=Adam(learning_rate=0.00001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# 6. 데이터 증강 설정\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.3,\n",
        "    height_shift_range=0.3,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.3,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# 데이터 증강 적용\n",
        "train_generator = datagen.flow(x_train, y_train, batch_size=32)\n",
        "\n",
        "# 7. 조기 종료 설정\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# 8. 모델 학습\n",
        "model.fit(\n",
        "    train_generator,\n",
        "    epochs=50,\n",
        "    validation_data=(x_test, y_test),\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "# 9. 모델 평가\n",
        "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
        "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n",
        "\n",
        "# 10. 예측 수행\n",
        "y_pred = model.predict(x_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rI4gmnQOf0mD",
        "outputId": "b9c5f539-f5b3-4aea-b7a1-8b64f2fb894f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 0us/step\n",
            "Epoch 1/50\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 77ms/step - accuracy: 0.1285 - loss: 3.8264 - val_accuracy: 0.3270 - val_loss: 2.0007\n",
            "Epoch 2/50\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 56ms/step - accuracy: 0.2149 - loss: 2.8566 - val_accuracy: 0.4520 - val_loss: 1.6613\n",
            "Epoch 3/50\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 55ms/step - accuracy: 0.3044 - loss: 2.3763 - val_accuracy: 0.5360 - val_loss: 1.3981\n",
            "Epoch 4/50\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 56ms/step - accuracy: 0.3720 - loss: 2.0786 - val_accuracy: 0.5969 - val_loss: 1.2085\n",
            "Epoch 5/50\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 55ms/step - accuracy: 0.4369 - loss: 1.8888 - val_accuracy: 0.6399 - val_loss: 1.0945\n",
            "Epoch 6/50\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 55ms/step - accuracy: 0.4834 - loss: 1.7354 - val_accuracy: 0.6753 - val_loss: 0.9797\n",
            "Epoch 7/50\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 56ms/step - accuracy: 0.5168 - loss: 1.6442 - val_accuracy: 0.7066 - val_loss: 0.8823\n",
            "Epoch 8/50\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 56ms/step - accuracy: 0.5451 - loss: 1.5185 - val_accuracy: 0.7264 - val_loss: 0.8346\n",
            "Epoch 9/50\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 55ms/step - accuracy: 0.5769 - loss: 1.4289 - val_accuracy: 0.7360 - val_loss: 0.8692\n",
            "Epoch 10/50\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 55ms/step - accuracy: 0.5920 - loss: 1.3590 - val_accuracy: 0.7498 - val_loss: 0.7878\n",
            "Epoch 11/50\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 55ms/step - accuracy: 0.6121 - loss: 1.2771 - val_accuracy: 0.7699 - val_loss: 0.6917\n",
            "Epoch 12/50\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 55ms/step - accuracy: 0.6313 - loss: 1.2270 - val_accuracy: 0.7724 - val_loss: 0.7352\n",
            "Epoch 13/50\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 55ms/step - accuracy: 0.6447 - loss: 1.1712 - val_accuracy: 0.7832 - val_loss: 0.6947\n",
            "Epoch 14/50\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 56ms/step - accuracy: 0.6560 - loss: 1.1104 - val_accuracy: 0.7923 - val_loss: 0.6292\n",
            "Epoch 15/50\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 56ms/step - accuracy: 0.6678 - loss: 1.0556 - val_accuracy: 0.7939 - val_loss: 0.6705\n",
            "Epoch 16/50\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 56ms/step - accuracy: 0.6772 - loss: 1.0398 - val_accuracy: 0.8065 - val_loss: 0.6663\n",
            "Epoch 17/50\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 56ms/step - accuracy: 0.6846 - loss: 1.0151 - val_accuracy: 0.8103 - val_loss: 0.5822\n",
            "Epoch 18/50\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 56ms/step - accuracy: 0.6978 - loss: 0.9744 - val_accuracy: 0.8184 - val_loss: 0.6099\n",
            "Epoch 19/50\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 55ms/step - accuracy: 0.7013 - loss: 0.9310 - val_accuracy: 0.8246 - val_loss: 0.5643\n",
            "Epoch 20/50\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 55ms/step - accuracy: 0.7061 - loss: 0.9132 - val_accuracy: 0.8293 - val_loss: 0.5910\n",
            "Epoch 21/50\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 55ms/step - accuracy: 0.7165 - loss: 0.8926 - val_accuracy: 0.8292 - val_loss: 0.6088\n",
            "Epoch 22/50\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 55ms/step - accuracy: 0.7194 - loss: 0.8744 - val_accuracy: 0.8316 - val_loss: 0.5581\n",
            "Epoch 23/50\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 55ms/step - accuracy: 0.7289 - loss: 0.8389 - val_accuracy: 0.8362 - val_loss: 0.5280\n",
            "Epoch 24/50\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 55ms/step - accuracy: 0.7370 - loss: 0.8145 - val_accuracy: 0.8431 - val_loss: 0.4914\n",
            "Epoch 25/50\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 55ms/step - accuracy: 0.7363 - loss: 0.8168 - val_accuracy: 0.8462 - val_loss: 0.4664\n",
            "Epoch 26/50\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 55ms/step - accuracy: 0.7403 - loss: 0.8044 - val_accuracy: 0.8461 - val_loss: 0.4867\n",
            "Epoch 27/50\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 56ms/step - accuracy: 0.7447 - loss: 0.7812 - val_accuracy: 0.8498 - val_loss: 0.4519\n",
            "Epoch 28/50\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 56ms/step - accuracy: 0.7517 - loss: 0.7553 - val_accuracy: 0.8523 - val_loss: 0.4320\n",
            "Epoch 29/50\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 55ms/step - accuracy: 0.7541 - loss: 0.7471 - val_accuracy: 0.8570 - val_loss: 0.4219\n",
            "Epoch 30/50\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 56ms/step - accuracy: 0.7578 - loss: 0.7468 - val_accuracy: 0.8562 - val_loss: 0.4303\n",
            "Epoch 31/50\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 55ms/step - accuracy: 0.7635 - loss: 0.7210 - val_accuracy: 0.8609 - val_loss: 0.4063\n",
            "Epoch 32/50\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 55ms/step - accuracy: 0.7665 - loss: 0.6990 - val_accuracy: 0.8670 - val_loss: 0.4021\n",
            "Epoch 33/50\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 55ms/step - accuracy: 0.7731 - loss: 0.6948 - val_accuracy: 0.8638 - val_loss: 0.4248\n",
            "Epoch 34/50\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 55ms/step - accuracy: 0.7787 - loss: 0.6760 - val_accuracy: 0.8708 - val_loss: 0.3803\n",
            "Epoch 35/50\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 55ms/step - accuracy: 0.7778 - loss: 0.6733 - val_accuracy: 0.8714 - val_loss: 0.3846\n",
            "Epoch 36/50\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 56ms/step - accuracy: 0.7840 - loss: 0.6629 - val_accuracy: 0.8718 - val_loss: 0.3772\n",
            "Epoch 37/50\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 55ms/step - accuracy: 0.7872 - loss: 0.6355 - val_accuracy: 0.8765 - val_loss: 0.3667\n",
            "Epoch 38/50\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 56ms/step - accuracy: 0.7833 - loss: 0.6507 - val_accuracy: 0.8754 - val_loss: 0.3671\n",
            "Epoch 39/50\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 56ms/step - accuracy: 0.7910 - loss: 0.6284 - val_accuracy: 0.8789 - val_loss: 0.3613\n",
            "Epoch 40/50\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 56ms/step - accuracy: 0.7953 - loss: 0.6153 - val_accuracy: 0.8806 - val_loss: 0.3657\n",
            "Epoch 41/50\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 56ms/step - accuracy: 0.7976 - loss: 0.6031 - val_accuracy: 0.8816 - val_loss: 0.3617\n",
            "Epoch 42/50\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 56ms/step - accuracy: 0.7978 - loss: 0.5988 - val_accuracy: 0.8840 - val_loss: 0.3435\n",
            "Epoch 43/50\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 55ms/step - accuracy: 0.8062 - loss: 0.5798 - val_accuracy: 0.8857 - val_loss: 0.3439\n",
            "Epoch 44/50\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 55ms/step - accuracy: 0.8096 - loss: 0.5671 - val_accuracy: 0.8863 - val_loss: 0.3395\n",
            "Epoch 45/50\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 55ms/step - accuracy: 0.8092 - loss: 0.5745 - val_accuracy: 0.8889 - val_loss: 0.3296\n",
            "Epoch 46/50\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 55ms/step - accuracy: 0.8107 - loss: 0.5586 - val_accuracy: 0.8900 - val_loss: 0.3217\n",
            "Epoch 47/50\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 55ms/step - accuracy: 0.8187 - loss: 0.5441 - val_accuracy: 0.8921 - val_loss: 0.3191\n",
            "Epoch 48/50\n",
            "\u001b[1m 860/1563\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 55ms/step - accuracy: 0.8212 - loss: 0.5449"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yiUJNmkZgMUq"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bd042_OagMdR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}