{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyN4I1/BUQdmTsh2ayVyILUp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimjaehwankimjaehwan/python_/blob/main/InceptionV3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFKzUka4aZ62",
        "outputId": "4784c2a7-26d9-4e34-f3ed-6344ca020f8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels.h5\n",
            "\u001b[1m96112376/96112376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 0us/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 12s/step\n",
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/imagenet_class_index.json\n",
            "\u001b[1m35363/35363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1us/step\n",
            "Predicted: [('n03891251', 'park_bench', 0.15180102), ('n03777754', 'modem', 0.049145788), ('n03223299', 'doormat', 0.04304461)]\n",
            "1: park_bench (0.15)\n",
            "2: modem (0.05)\n",
            "3: doormat (0.04)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_sample_image\n",
        "from tensorflow.keras.applications import InceptionV3\n",
        "from tensorflow.keras.applications.inception_v3 import preprocess_input, decode_predictions\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "\n",
        "# 1.  모델 로드\n",
        "model = InceptionV3(weights='imagenet')\n",
        "\n",
        "# 2. sklearn에서 제공하는 샘플 이미지 로드\n",
        "# 사용 가능한 샘플 이미지: 'china.jpg', 'flower.jpg'\n",
        "image = load_sample_image('china.jpg')  # 또는 'flower.jpg'\n",
        "image = img_to_array(image)\n",
        "\n",
        "# 3. 이미지 전처리\n",
        "# InceptionV3 모델에 입력할 수 있도록 이미지 크기를 299x299로 조정하고, 전처리\n",
        "image = np.resize(image, (299, 299, 3))\n",
        "image = np.expand_dims(image, axis=0)\n",
        "image = preprocess_input(image)\n",
        "\n",
        "# 4. 예측 수행\n",
        "predictions = model.predict(image)\n",
        "\n",
        "# 5. 예측 결과 디코딩 및 출력\n",
        "decoded_predictions = decode_predictions(predictions, top=3)[0]\n",
        "print(\"Predicted:\", decoded_predictions)\n",
        "\n",
        "# 출력된 결과를 해석\n",
        "for i, (imagenet_id, label, score) in enumerate(decoded_predictions):\n",
        "    print(f\"{i+1}: {label} ({score:.2f})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. park_bench (0.15):\n",
        "\n",
        "  모델은 이미지가 \"park bench\" (공원 벤치)일 가능성을 약 15.18%로 예측했습니다.\n",
        "  이 예측이 가장 높은 확률로, 모델이 이미지에서 공원 벤치와 관련된 특징을 인식했을 가능성이 큽니다.\n",
        "\n",
        "2. modem (0.05):\n",
        "\n",
        "  모델은 이미지가 \"modem\" (모뎀)일 가능성을 약 4.91%로 예측했습니다.\n",
        "  이는 이미지에서 모뎀과 관련된 특징이 일부 인식되었음을 시사합니다.\n",
        "\n",
        "3. doormat (0.04):\n",
        "\n",
        "  모델은 이미지가 \"doormat\" (현관 매트)일 가능성을 약 4.30%로 예측했습니다.\n",
        "  이미지에서 현관 매트와 유사한 특징이 발견되었을 가능성이 있습니다."
      ],
      "metadata": {
        "id": "pyXkH3fpbqoV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_sample_image\n",
        "from tensorflow.keras.applications import InceptionV3\n",
        "from tensorflow.keras.applications.inception_v3 import preprocess_input, decode_predictions\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, ImageDataGenerator\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1. InceptionV3 모델 로드 및 미세 조정 설정\n",
        "base_model = InceptionV3(weights='imagenet', include_top=False)  # 최상위 레이어 제거\n",
        "\n",
        "# 2. 레이어 추가 및 모델 확장\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)  # 드롭아웃 추가\n",
        "predictions = Dense(1000, activation='softmax')(x)  # ImageNet 데이터셋의 클래스 수\n",
        "\n",
        "# 새 모델 정의\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# 3. 일부 레이어 동결 해제 (Fine-tuning)\n",
        "for layer in base_model.layers[-50:]:  # 마지막 50개 레이어는 학습 가능하도록 설정\n",
        "    layer.trainable = True\n",
        "\n",
        "# 4. 모델 컴파일 (하이퍼파라미터 튜닝)\n",
        "model.compile(optimizer=Adam(learning_rate=0.00001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# 5. 데이터 증강 설정\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest',\n",
        "    validation_split=0.2  # 데이터의 20%를 검증 데이터로 사용\n",
        ")\n",
        "\n",
        "# 6. 샘플 이미지 로드 및 전처리 (여기서는 데이터 증강 예시로 사용)\n",
        "# 실제로는 여러 이미지를 사용해야 하므로, 아래는 데이터 로딩의 예시입니다.\n",
        "image = load_sample_image('china.jpg')  # 예제 이미지\n",
        "image = img_to_array(image)\n",
        "image = np.resize(image, (299, 299, 3))  # InceptionV3 입력 크기에 맞추기\n",
        "image = np.expand_dims(image, axis=0)\n",
        "image = preprocess_input(image)\n",
        "\n",
        "# 7. 가상 데이터를 사용한 데이터 증강\n",
        "# 실제로는 여러 샘플을 로드하고 데이터셋을 구축해야 합니다.\n",
        "train_data, val_data, train_labels, val_labels = train_test_split([image]*100, to_categorical([1]*100, 1000), test_size=0.2)\n",
        "\n",
        "train_data = np.squeeze(train_data, axis=1) # Remove the extra dimension from train_data\n",
        "val_data = np.squeeze(val_data, axis=1) # Remove the extra dimension from val_data\n",
        "\n",
        "train_generator = datagen.flow(\n",
        "    np.array(train_data),\n",
        "    np.array(train_labels),\n",
        "    batch_size=32,\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "val_generator = datagen.flow(\n",
        "    np.array(val_data),\n",
        "    np.array(val_labels),\n",
        "    batch_size=32,\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "# 조기 종료 설정\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# 8. 모델 학습\n",
        "model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=50,\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "# 9. 모델 평가 (검증 데이터에서)\n",
        "val_loss, val_accuracy = model.evaluate(val_generator)\n",
        "print(f\"Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}\")\n",
        "\n",
        "# 10. 예측 수행 (여기서는 샘플 이미지로 예시)\n",
        "y_pred = model.predict(image)\n",
        "decoded_predictions = decode_predictions(y_pred, top=3)[0]\n",
        "print(\"Predicted:\", decoded_predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPkt553icA5b",
        "outputId": "caaf3739-290d-4ec1-efa0-f56827f7b6a1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 12s/step - accuracy: 0.0000e+00 - loss: 6.6315 - val_accuracy: 0.0000e+00 - val_loss: 6.5073\n",
            "Epoch 2/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 312ms/step - accuracy: 0.0000e+00 - loss: 6.5239 - val_accuracy: 0.0000e+00 - val_loss: 6.2298\n",
            "Epoch 3/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 297ms/step - accuracy: 0.0312 - loss: 6.3609 - val_accuracy: 0.0000e+00 - val_loss: 6.2129\n",
            "Epoch 4/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 302ms/step - accuracy: 0.0312 - loss: 6.2156 - val_accuracy: 1.0000 - val_loss: 5.7569\n",
            "Epoch 5/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 301ms/step - accuracy: 0.2396 - loss: 5.9154 - val_accuracy: 1.0000 - val_loss: 5.7286\n",
            "Epoch 6/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 310ms/step - accuracy: 0.4167 - loss: 5.7153 - val_accuracy: 1.0000 - val_loss: 5.4394\n",
            "Epoch 7/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 176ms/step - accuracy: 0.4062 - loss: 5.6884 - val_accuracy: 0.7500 - val_loss: 5.4589\n",
            "Epoch 8/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 296ms/step - accuracy: 0.6354 - loss: 5.4693 - val_accuracy: 1.0000 - val_loss: 5.0862\n",
            "Epoch 9/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 303ms/step - accuracy: 0.8646 - loss: 5.2329 - val_accuracy: 1.0000 - val_loss: 4.7980\n",
            "Epoch 10/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 318ms/step - accuracy: 0.9271 - loss: 5.0858 - val_accuracy: 1.0000 - val_loss: 4.6032\n",
            "Epoch 11/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 299ms/step - accuracy: 0.9479 - loss: 4.9803 - val_accuracy: 1.0000 - val_loss: 4.3676\n",
            "Epoch 12/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 298ms/step - accuracy: 1.0000 - loss: 4.6972 - val_accuracy: 1.0000 - val_loss: 3.9786\n",
            "Epoch 13/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 303ms/step - accuracy: 1.0000 - loss: 4.5702 - val_accuracy: 1.0000 - val_loss: 3.6558\n",
            "Epoch 14/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 176ms/step - accuracy: 1.0000 - loss: 4.4181 - val_accuracy: 1.0000 - val_loss: 3.7837\n",
            "Epoch 15/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 305ms/step - accuracy: 1.0000 - loss: 4.1958 - val_accuracy: 1.0000 - val_loss: 3.5078\n",
            "Epoch 16/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 297ms/step - accuracy: 1.0000 - loss: 4.0954 - val_accuracy: 1.0000 - val_loss: 3.2603\n",
            "Epoch 17/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 315ms/step - accuracy: 1.0000 - loss: 3.8154 - val_accuracy: 1.0000 - val_loss: 2.8616\n",
            "Epoch 18/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 177ms/step - accuracy: 1.0000 - loss: 3.5831 - val_accuracy: 1.0000 - val_loss: 2.9595\n",
            "Epoch 19/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 307ms/step - accuracy: 1.0000 - loss: 3.5354 - val_accuracy: 1.0000 - val_loss: 2.5197\n",
            "Epoch 20/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 300ms/step - accuracy: 1.0000 - loss: 3.2939 - val_accuracy: 1.0000 - val_loss: 2.2887\n",
            "Epoch 21/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 308ms/step - accuracy: 1.0000 - loss: 3.1245 - val_accuracy: 1.0000 - val_loss: 2.2194\n",
            "Epoch 22/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 177ms/step - accuracy: 1.0000 - loss: 2.9451 - val_accuracy: 1.0000 - val_loss: 2.2576\n",
            "Epoch 23/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 306ms/step - accuracy: 1.0000 - loss: 2.7738 - val_accuracy: 1.0000 - val_loss: 1.7251\n",
            "Epoch 24/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 176ms/step - accuracy: 1.0000 - loss: 2.6107 - val_accuracy: 1.0000 - val_loss: 1.8140\n",
            "Epoch 25/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 313ms/step - accuracy: 1.0000 - loss: 2.4130 - val_accuracy: 1.0000 - val_loss: 1.2511\n",
            "Epoch 26/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 308ms/step - accuracy: 1.0000 - loss: 2.1666 - val_accuracy: 1.0000 - val_loss: 1.1077\n",
            "Epoch 27/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 311ms/step - accuracy: 1.0000 - loss: 2.1005 - val_accuracy: 1.0000 - val_loss: 0.8813\n",
            "Epoch 28/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 176ms/step - accuracy: 1.0000 - loss: 1.9602 - val_accuracy: 1.0000 - val_loss: 1.0069\n",
            "Epoch 29/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 309ms/step - accuracy: 1.0000 - loss: 1.7222 - val_accuracy: 1.0000 - val_loss: 0.8812\n",
            "Epoch 30/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 312ms/step - accuracy: 1.0000 - loss: 1.5694 - val_accuracy: 1.0000 - val_loss: 0.7938\n",
            "Epoch 31/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 299ms/step - accuracy: 1.0000 - loss: 1.5211 - val_accuracy: 1.0000 - val_loss: 0.3546\n",
            "Epoch 32/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 173ms/step - accuracy: 1.0000 - loss: 1.3597 - val_accuracy: 1.0000 - val_loss: 0.6795\n",
            "Epoch 33/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 317ms/step - accuracy: 1.0000 - loss: 1.1417 - val_accuracy: 1.0000 - val_loss: 0.3318\n",
            "Epoch 34/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 183ms/step - accuracy: 1.0000 - loss: 1.0886 - val_accuracy: 1.0000 - val_loss: 0.3692\n",
            "Epoch 35/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 314ms/step - accuracy: 1.0000 - loss: 0.9413 - val_accuracy: 1.0000 - val_loss: 0.3048\n",
            "Epoch 36/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 300ms/step - accuracy: 1.0000 - loss: 0.8217 - val_accuracy: 1.0000 - val_loss: 0.1921\n",
            "Epoch 37/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 306ms/step - accuracy: 1.0000 - loss: 0.7533 - val_accuracy: 1.0000 - val_loss: 0.1786\n",
            "Epoch 38/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 318ms/step - accuracy: 1.0000 - loss: 0.6586 - val_accuracy: 1.0000 - val_loss: 0.1701\n",
            "Epoch 39/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 321ms/step - accuracy: 1.0000 - loss: 0.5687 - val_accuracy: 1.0000 - val_loss: 0.1434\n",
            "Epoch 40/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 306ms/step - accuracy: 1.0000 - loss: 0.5359 - val_accuracy: 1.0000 - val_loss: 0.1063\n",
            "Epoch 41/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 178ms/step - accuracy: 1.0000 - loss: 0.4832 - val_accuracy: 1.0000 - val_loss: 0.1128\n",
            "Epoch 42/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 186ms/step - accuracy: 1.0000 - loss: 0.4016 - val_accuracy: 1.0000 - val_loss: 0.1451\n",
            "Epoch 43/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 313ms/step - accuracy: 1.0000 - loss: 0.4048 - val_accuracy: 1.0000 - val_loss: 0.0815\n",
            "Epoch 44/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 309ms/step - accuracy: 1.0000 - loss: 0.3672 - val_accuracy: 1.0000 - val_loss: 0.0440\n",
            "Epoch 45/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 176ms/step - accuracy: 1.0000 - loss: 0.3177 - val_accuracy: 1.0000 - val_loss: 0.0645\n",
            "Epoch 46/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 185ms/step - accuracy: 1.0000 - loss: 0.2724 - val_accuracy: 1.0000 - val_loss: 0.0937\n",
            "Epoch 47/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 178ms/step - accuracy: 1.0000 - loss: 0.2512 - val_accuracy: 1.0000 - val_loss: 0.0638\n",
            "Epoch 48/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 179ms/step - accuracy: 1.0000 - loss: 0.2184 - val_accuracy: 1.0000 - val_loss: 0.0907\n",
            "Epoch 49/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 182ms/step - accuracy: 1.0000 - loss: 0.2365 - val_accuracy: 1.0000 - val_loss: 0.0658\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 1.0000 - loss: 0.0608\n",
            "Validation Loss: 0.06084834784269333, Validation Accuracy: 1.0\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10s/step\n",
            "Predicted: [('n01443537', 'goldfish', 0.73273593), ('n02692877', 'airship', 0.0009896001), ('n02098105', 'soft-coated_wheaten_terrier', 0.0009248814)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델이 성공적으로 학습되었고, 검증 데이터에서 매우 높은 정확도와 낮은 손실 값을 달성한 것을 볼 수 있습니다. 마지막 예측 결과에서 확률이 높은 \"goldfish\" 클래스를 예측한 것을 보면, 모델이 입력 이미지를 잘 학습하고 예측하는 것을 확인할 수 있습니다."
      ],
      "metadata": {
        "id": "XugYhCA3dM5B"
      }
    }
  ]
}