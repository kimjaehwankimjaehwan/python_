{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyONUFKaywMgJUVFJ1ZsGWNv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimjaehwankimjaehwan/python_/blob/main/ResNet50.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. ResNet50 모델 로드:\n",
        "\n",
        "ResNet50 모델은 사전 학습된 가중치(weights='imagenet')로 로드됩니다. 이 모델은 ImageNet 데이터셋에서 학습된 모델로, 1000개의 클래스를 예측할 수 있습니다.\n",
        "\n",
        "2. 이미지 로드:\n",
        "\n",
        "sklearn에서 제공하는 샘플 이미지를 사용합니다. load_sample_image('china.jpg') 또는 load_sample_image('flower.jpg')로 이미지를 로드할 수 있습니다.\n",
        "\n",
        "3. 이미지 전처리:\n",
        "\n",
        "이미지를 ResNet50 모델에 맞는 입력 크기인 224x224로 조정합니다. 이때 이미지 배열을 numpy.resize를 사용해 조정합니다.\n",
        "전처리 함수 preprocess_input을 사용하여 이미지 데이터를 ResNet50이 기대하는 형식으로 변환합니다. 이 함수는 픽셀 값을 적절한 범위로 조정합니다.\n",
        "\n",
        "4. 예측 수행:\n",
        "\n",
        "model.predict(image)를 사용하여 예측을 수행합니다. 이 모델은 입력된 이미지에 대한 예측 확률을 반환합니다.\n",
        "\n",
        "5. 예측 결과 디코딩:\n",
        "\n",
        "decode_predictions를 사용하여 예측 결과를 해석 가능한 레이블로 디코딩합니다. top=3을 사용하여 가장 가능성이 높은 3개의 클래스를 출력합니다.\n",
        "\n",
        "6. 결과 출력:\n",
        "\n",
        "각 예측 결과에 대해 레이블과 해당 확률을 출력합니다."
      ],
      "metadata": {
        "id": "Z01JR6GSYm3d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtXK7PoBYa_V",
        "outputId": "5f36b76a-53aa-421b-9b25-839e9b76c44d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels.h5\n",
            "\u001b[1m102967424/102967424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 0us/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7s/step\n",
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/imagenet_class_index.json\n",
            "\u001b[1m35363/35363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Predicted: [('n04152593', 'screen', 0.10869243), ('n02840245', 'binder', 0.104571015), ('n06359193', 'web_site', 0.049777426)]\n",
            "1: screen (0.11)\n",
            "2: binder (0.10)\n",
            "3: web_site (0.05)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_sample_image\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "\n",
        "# 1. ResNet50 모델 로드\n",
        "model = ResNet50(weights='imagenet')\n",
        "\n",
        "# 2. sklearn에서 제공하는 샘플 이미지 로드\n",
        "# 사용 가능한 샘플 이미지: 'china.jpg', 'flower.jpg'\n",
        "image = load_sample_image('china.jpg')  # 또는 'flower.jpg'\n",
        "image = img_to_array(image)\n",
        "\n",
        "# 3. 이미지 전처리\n",
        "# ResNet50 모델에 입력할 수 있도록 이미지 크기를 224x224로 조정하고, 전처리\n",
        "image = np.resize(image, (224, 224, 3))\n",
        "image = np.expand_dims(image, axis=0)\n",
        "image = preprocess_input(image)\n",
        "\n",
        "# 4. 예측 수행\n",
        "predictions = model.predict(image)\n",
        "\n",
        "# 5. 예측 결과 디코딩 및 출력\n",
        "decoded_predictions = decode_predictions(predictions, top=3)[0]\n",
        "print(\"Predicted:\", decoded_predictions)\n",
        "\n",
        "# 출력된 결과를 해석\n",
        "for i, (imagenet_id, label, score) in enumerate(decoded_predictions):\n",
        "    print(f\"{i+1}: {label} ({score:.2f})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* screen (0.11):\n",
        "\n",
        "  모델이 이미지에서 \"스크린(screen)\"이라고 예측한 확률이 약 11%입니다.\n",
        "이 예측이 가장 높은 확률로, 모델은 이미지가 스크린과 관련이 있을 가능성이 있다고 판단했습니다.\n",
        "\n",
        "* binder (0.10):\n",
        "\n",
        "  모델은 또한 이미지가 \"바인더(binder)\"일 가능성을 약 10%로 예측했습니다.\n",
        "이는 바인더(서류철)와 관련된 물체를 인식한 것으로 보입니다.\n",
        "\n",
        "* web_site (0.05):\n",
        "\n",
        "  세 번째로, 모델은 이미지가 \"웹 사이트(web site)\"와 관련이 있을 가능성을 약 5%로 예측했습니다.\n",
        "  이 결과는 이미지에 웹 페이지와 관련된 요소가 있을 수 있다고 인식한 결과입니다."
      ],
      "metadata": {
        "id": "g7-px2CWY89U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_sample_image\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img, ImageDataGenerator\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# 1. ResNet50 모델 로드 및 미세 조정 설정\n",
        "base_model = ResNet50(weights='imagenet', include_top=False)  # 최상위 레이어 제거\n",
        "\n",
        "# 2. 레이어 추가 및 모델 확장\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)  # 드롭아웃 추가\n",
        "predictions = Dense(1000, activation='softmax')(x)  # ImageNet 데이터셋의 클래스 수\n",
        "\n",
        "# 새 모델 정의\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# 3. 사전 학습된 레이어 동결\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False  # ResNet50의 기존 레이어는 학습되지 않도록 동결\n",
        "\n",
        "# 4. 모델 컴파일\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# 5. 데이터 증강 설정\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# 6. 샘플 이미지 로드 및 전처리\n",
        "image = load_sample_image('china.jpg')  # 예제 이미지\n",
        "image = img_to_array(image)\n",
        "image = np.resize(image, (224, 224, 3))  # ResNet50 입력 크기에 맞추기\n",
        "image = np.expand_dims(image, axis=0)\n",
        "image = preprocess_input(image)\n",
        "\n",
        "# 7. 예측 수행\n",
        "y_pred = model.predict(image)\n",
        "\n",
        "# 8. 예측 결과 디코딩 및 출력\n",
        "decoded_predictions = decode_predictions(y_pred, top=3)[0]\n",
        "print(\"Predicted:\", decoded_predictions)\n",
        "\n",
        "# 9. 모델 학습 (이 부분은 예제에서 생략됩니다)\n",
        "# 일반적으로 대규모 데이터셋을 사용하여 학습을 진행해야 합니다.\n",
        "# datagen.flow(...) 또는 model.fit(...)을 사용하여 데이터 증강된 이미지로 학습 진행 가능\n",
        "\n",
        "# 조기 종료 설정\n",
        "# early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# 실제 데이터로 모델 학습\n",
        "# model.fit(train_data, validation_data=val_data, epochs=50, callbacks=[early_stopping])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FyXDC2_YZeSg",
        "outputId": "ee9c0a14-4ac4-4285-eb85-4cde8dad317b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 0us/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6s/step\n",
            "Predicted: [('n02782093', 'balloon', 0.0087746605), ('n03697007', 'lumbermill', 0.00767095), ('n03478589', 'half_track', 0.0074001565)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* balloon (0.0088):\n",
        "모델은 이미지의 일부를 \"balloon\" (풍선)이라고 인식했을 가능성을 약 0.88%로 예측했습니다.\n",
        "* lumbermill (0.0077):\n",
        "모델은 이미지의 일부를 \"lumbermill\" (제재소)라고 인식할 가능성을 약 0.77%로 예측했습니다.\n",
        "* half_track (0.0074):\n",
        "모델은 이미지의 일부를 \"half_track\" (반트랙, 장갑차의 일종)으로 인식할 가능성을 약 0.74%로 예측했습니다."
      ],
      "metadata": {
        "id": "IZHno8BjZ-Gx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_sample_image\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, ImageDataGenerator\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1. ResNet50 모델 로드 및 미세 조정 설정\n",
        "base_model = ResNet50(weights='imagenet', include_top=False)  # 최상위 레이어 제거\n",
        "\n",
        "# 2. 레이어 추가 및 모델 확장\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)  # 드롭아웃 추가\n",
        "predictions = Dense(1000, activation='softmax')(x)  # ImageNet 데이터셋의 클래스 수\n",
        "\n",
        "# 새 모델 정의\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# 3. 일부 레이어 동결 해제 (Fine-tuning)\n",
        "for layer in base_model.layers[-50:]:  # 마지막 50개 레이어는 학습 가능하도록 설정\n",
        "    layer.trainable = True\n",
        "\n",
        "# 4. 모델 컴파일 (하이퍼파라미터 튜닝)\n",
        "model.compile(optimizer=Adam(learning_rate=0.00001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# 5. 데이터 증강 설정\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest',\n",
        "    validation_split=0.2  # 데이터의 20%를 검증 데이터로 사용\n",
        ")\n",
        "\n",
        "# 6. 샘플 이미지 로드 및 전처리 (여기서는 데이터 증강 예시로 사용)\n",
        "# 실제로는 여러 이미지를 사용해야 하므로, 아래는 데이터 로딩의 예시입니다.\n",
        "image = load_sample_image('china.jpg')  # 예제 이미지\n",
        "image = img_to_array(image)\n",
        "image = np.resize(image, (224, 224, 3))  # ResNet50 입력 크기에 맞추기\n",
        "image = np.expand_dims(image, axis=0)\n",
        "image = preprocess_input(image)\n",
        "\n",
        "# 7. 데이터 증강을 사용한 학습\n",
        "# 실제 데이터가 더 많아야 하며, 여기에 추가하여 증강된 이미지를 사용합니다.\n",
        "train_data, val_data, train_labels, val_labels = train_test_split([image]*100, to_categorical([1]*100, 1000), test_size=0.2)\n",
        "\n",
        "# Remove the extra dimension from train_data and val_data\n",
        "train_data = np.squeeze(train_data, axis=1)\n",
        "val_data = np.squeeze(val_data, axis=1)\n",
        "\n",
        "train_generator = datagen.flow(\n",
        "    np.array(train_data),\n",
        "    np.array(train_labels),\n",
        "    batch_size=32,\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "val_generator = datagen.flow(\n",
        "    np.array(val_data),\n",
        "    np.array(val_labels),\n",
        "    batch_size=32,\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "# 조기 종료 설정\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# 모델 학습\n",
        "model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=50,\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "# 8. 모델 평가 (검증 데이터에서)\n",
        "val_loss, val_accuracy = model.evaluate(val_generator)\n",
        "print(f\"Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}\")\n",
        "\n",
        "# 9. 예측 수행 (여기서는 샘플 이미지로 예시)\n",
        "y_pred = model.predict(image)\n",
        "decoded_predictions = decode_predictions(y_pred, top=3)[0]\n",
        "print(\"Predicted:\", decoded_predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-BzD_Pda8Vg",
        "outputId": "90804835-f9e6-42bb-ce27-ee742952e88b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 8s/step - accuracy: 0.0000e+00 - loss: 6.5011 - val_accuracy: 0.0000e+00 - val_loss: 6.2487\n",
            "Epoch 2/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 249ms/step - accuracy: 0.0208 - loss: 6.2271 - val_accuracy: 0.0000e+00 - val_loss: 5.9280\n",
            "Epoch 3/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 247ms/step - accuracy: 0.0729 - loss: 5.9809 - val_accuracy: 0.0000e+00 - val_loss: 5.6887\n",
            "Epoch 4/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 132ms/step - accuracy: 0.0938 - loss: 5.8532 - val_accuracy: 0.2500 - val_loss: 5.8215\n",
            "Epoch 5/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 241ms/step - accuracy: 0.1250 - loss: 5.5249 - val_accuracy: 0.0000e+00 - val_loss: 5.5861\n",
            "Epoch 6/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 243ms/step - accuracy: 0.3333 - loss: 5.1115 - val_accuracy: 0.2500 - val_loss: 5.3375\n",
            "Epoch 7/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 256ms/step - accuracy: 0.4688 - loss: 4.9347 - val_accuracy: 0.7500 - val_loss: 5.0371\n",
            "Epoch 8/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 135ms/step - accuracy: 0.6354 - loss: 4.5631 - val_accuracy: 0.2500 - val_loss: 5.2265\n",
            "Epoch 9/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 132ms/step - accuracy: 0.8125 - loss: 4.3162 - val_accuracy: 0.7500 - val_loss: 5.0625\n",
            "Epoch 10/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 247ms/step - accuracy: 0.7812 - loss: 4.1363 - val_accuracy: 0.7500 - val_loss: 4.8493\n",
            "Epoch 11/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 135ms/step - accuracy: 0.8542 - loss: 3.7879 - val_accuracy: 0.7500 - val_loss: 4.8923\n",
            "Epoch 12/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 245ms/step - accuracy: 0.9271 - loss: 3.6062 - val_accuracy: 1.0000 - val_loss: 4.5229\n",
            "Epoch 13/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 246ms/step - accuracy: 0.9792 - loss: 3.3413 - val_accuracy: 1.0000 - val_loss: 4.0933\n",
            "Epoch 14/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 138ms/step - accuracy: 0.9896 - loss: 3.0297 - val_accuracy: 1.0000 - val_loss: 4.2369\n",
            "Epoch 15/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 248ms/step - accuracy: 0.9792 - loss: 2.7536 - val_accuracy: 1.0000 - val_loss: 3.8397\n",
            "Epoch 16/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 133ms/step - accuracy: 1.0000 - loss: 2.5370 - val_accuracy: 1.0000 - val_loss: 3.8758\n",
            "Epoch 17/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 246ms/step - accuracy: 1.0000 - loss: 2.1810 - val_accuracy: 1.0000 - val_loss: 3.6245\n",
            "Epoch 18/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 133ms/step - accuracy: 1.0000 - loss: 2.1010 - val_accuracy: 1.0000 - val_loss: 3.8194\n",
            "Epoch 19/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 260ms/step - accuracy: 1.0000 - loss: 1.8995 - val_accuracy: 1.0000 - val_loss: 3.3532\n",
            "Epoch 20/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 134ms/step - accuracy: 1.0000 - loss: 1.5561 - val_accuracy: 1.0000 - val_loss: 3.6316\n",
            "Epoch 21/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 243ms/step - accuracy: 1.0000 - loss: 1.3486 - val_accuracy: 1.0000 - val_loss: 2.8064\n",
            "Epoch 22/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 258ms/step - accuracy: 1.0000 - loss: 1.3387 - val_accuracy: 1.0000 - val_loss: 1.9180\n",
            "Epoch 23/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 137ms/step - accuracy: 1.0000 - loss: 1.0875 - val_accuracy: 1.0000 - val_loss: 2.2505\n",
            "Epoch 24/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 241ms/step - accuracy: 1.0000 - loss: 1.0047 - val_accuracy: 1.0000 - val_loss: 1.6655\n",
            "Epoch 25/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 130ms/step - accuracy: 1.0000 - loss: 0.8595 - val_accuracy: 1.0000 - val_loss: 1.7029\n",
            "Epoch 26/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 136ms/step - accuracy: 1.0000 - loss: 0.6620 - val_accuracy: 1.0000 - val_loss: 2.1591\n",
            "Epoch 27/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 133ms/step - accuracy: 1.0000 - loss: 0.6152 - val_accuracy: 1.0000 - val_loss: 2.7430\n",
            "Epoch 28/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 132ms/step - accuracy: 1.0000 - loss: 0.4817 - val_accuracy: 1.0000 - val_loss: 2.6630\n",
            "Epoch 29/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 133ms/step - accuracy: 1.0000 - loss: 0.4529 - val_accuracy: 1.0000 - val_loss: 2.3790\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 1.0000 - loss: 1.4076\n",
            "Validation Loss: 1.4076075553894043, Validation Accuracy: 1.0\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Predicted: [('n01443537', 'goldfish', 0.021815894), ('n04074963', 'remote_control', 0.0051372373), ('n02389026', 'sorrel', 0.004326279)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. ResNet50 모델 로드 및 Fine-tuning:\n",
        "\n",
        "  ResNet50 모델의 마지막 50개 레이어를 학습 가능하도록 설정하여 더 깊이 Fine-tuning을 진행합니다.\n",
        "\n",
        "2. 모델 확장 및 드롭아웃 적용:\n",
        "\n",
        "  GlobalAveragePooling2D, Dense, Dropout 레이어를 추가하여 모델을 확장합니다.\n",
        "  드롭아웃을 사용하여 과적합을 방지합니다.\n",
        "\n",
        "3. 데이터 증강 (Data Augmentation):\n",
        "\n",
        "  다양한 이미지 증강 기법을 적용하여 학습 데이터의 다양성을 증가시킵니다.\n",
        "  ImageDataGenerator를 사용하여 이미지 데이터를 실시간으로 증강합니다.\n",
        "\n",
        "4. 모델 컴파일 및 하이퍼파라미터 튜닝:\n",
        "\n",
        "  학습률(learning rate)을 0.00001로 설정하여 최적화합니다.\n",
        "  categorical_crossentropy 손실 함수를 사용하여 다중 클래스 분류 작업을 수행합니다.\n",
        "5. 모델 학습 및 평가:\n",
        "\n",
        "  학습 과정에서 조기 종료(Early Stopping)를 사용하여 과적합을 방지합니다.\n",
        "  학습 후 검증 데이터셋에서 모델을 평가하여 성능을 확인합니다.\n",
        "\n",
        "6. 예측 수행:\n",
        "\n",
        "  모델을 사용하여 예측을 수행하고, 결과를 디코딩하여 출력합니다."
      ],
      "metadata": {
        "id": "4VV_8EjnbKEZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델이 성공적으로 학습된 후, 검증 데이터에서 100%의 정확도(accuracy)를 달성한 것을 볼 수 있습니다. 그러나 모델의 val_loss 값이 학습이 진행됨에 따라 다시 증가하는 현상이 나타났습니다. 이는 모델이 검증 데이터에 대해 과적합(overfitting)되고 있을 가능성이 있습니다. 또한, 예측 결과에서 보이는 확률들이 상대적으로 낮은 것을 볼 때, 모델이 완벽히 학습되지 않았을 가능성도 있습니다."
      ],
      "metadata": {
        "id": "tYtfvjkyce18"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import BatchNormalization\n",
        "\n",
        "# 2. 레이어 추가 및 모델 확장 (드롭아웃 비율 증가 및 배치 정규화 추가)\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "x = BatchNormalization()(x)  # 배치 정규화 추가\n",
        "x = Dropout(0.6)(x)  # 드롭아웃 비율 증가\n",
        "predictions = Dense(1000, activation='softmax')(x)  # ImageNet 데이터셋의 클래스 수\n",
        "\n",
        "# 모델 재정의\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# 4. 모델 컴파일 (학습률 조정)\n",
        "model.compile(optimizer=Adam(learning_rate=0.000005), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# 조기 종료 설정 (patience 감소)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "# 모델 학습\n",
        "model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=50,\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "# 모델 평가 및 예측은 동일\n",
        "val_loss, val_accuracy = model.evaluate(val_generator)\n",
        "print(f\"Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}\")\n",
        "\n",
        "# 예측 수행 (여기서는 샘플 이미지로 예시)\n",
        "y_pred = model.predict(image)\n",
        "decoded_predictions = decode_predictions(y_pred, top=3)[0]\n",
        "print(\"Predicted:\", decoded_predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eeNq5yizbP1f",
        "outputId": "6d44c3e6-7ebc-4384-8050-66cef8e02bb5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 4s/step - accuracy: 0.0000e+00 - loss: 8.0329 - val_accuracy: 0.0000e+00 - val_loss: 7.3382\n",
            "Epoch 2/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 136ms/step - accuracy: 0.0000e+00 - loss: 8.0584 - val_accuracy: 0.0000e+00 - val_loss: 7.4073\n",
            "Epoch 3/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 134ms/step - accuracy: 0.0000e+00 - loss: 7.8042 - val_accuracy: 0.0000e+00 - val_loss: 7.3833\n",
            "Epoch 4/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 264ms/step - accuracy: 0.0000e+00 - loss: 8.1724 - val_accuracy: 0.0000e+00 - val_loss: 7.2613\n",
            "Epoch 5/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 136ms/step - accuracy: 0.0000e+00 - loss: 8.0753 - val_accuracy: 0.0000e+00 - val_loss: 7.2982\n",
            "Epoch 6/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 137ms/step - accuracy: 0.0000e+00 - loss: 8.0669 - val_accuracy: 0.0000e+00 - val_loss: 7.4339\n",
            "Epoch 7/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 137ms/step - accuracy: 0.0000e+00 - loss: 8.0783 - val_accuracy: 0.0000e+00 - val_loss: 7.2700\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.0000e+00 - loss: 7.2992\n",
            "Validation Loss: 7.299230098724365, Validation Accuracy: 0.0\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Predicted: [('n02167151', 'ground_beetle', 0.0047022626), ('n02895154', 'breastplate', 0.004209844), ('n04120489', 'running_shoe', 0.00403981)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. 데이터셋 크기 증가:\n",
        "\n",
        "  현재 모델은 소량의 데이터로 학습된 것으로 보입니다. 더 많은 데이터를 사용해 모델을 학습시키면 과적합을 줄일 수 있습니다. 더 많은 데이터를 수집하거나, 데이터 증강을 통해 가상 데이터를 생성하는 방법을 사용할 수 있습니다.\n",
        "\n",
        "2. 드롭아웃(Dropout) 레이어 추가 또는 비율 조정:\n",
        "\n",
        "  과적합을 줄이기 위해 드롭아웃 레이어의 비율을 높이거나 추가 드롭아웃 레이어를 모델에 추가할 수 있습니다. 현재 설정된 드롭아웃 비율을 0.5에서 0.6이나 0.7로 조정해 볼 수 있습니다.\n",
        "\n",
        "3. 학습률 조정:\n",
        "\n",
        "  학습률을 더 낮게 설정하여 모델이 더 천천히, 더욱 정밀하게 학습할 수 있도록 합니다. 이는 과적합을 줄이는 데 도움을 줄 수 있습니다.\n",
        "\n",
        "4. 얼리 스토핑(Early Stopping) 전략 개선:\n",
        "\n",
        "  검증 손실(val_loss)을 모니터링하여 더 일찍 학습을 중단할 수 있도록 조기 종료 조건을 더욱 엄격하게 설정할 수 있습니다. patience 매개변수를 줄여 조기 종료를 더 빠르게 할 수 있습니다."
      ],
      "metadata": {
        "id": "iYxgoT1_cwQV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "현재의 학습 결과를 보면, 모델의 학습이 제대로 진행되지 않은 것으로 보입니다. 모델의 accuracy와 val_accuracy가 0으로 유지되고 있으며, loss와 val_loss가 거의 변화하지 않고 있습니다. 이는 모델이 데이터를 제대로 학습하지 못하고 있음을 의미합니다."
      ],
      "metadata": {
        "id": "bUmakf2Pdg5K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# 모델의 복잡성 줄이기 (Dense 레이어와 뉴런 수 감소)\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "predictions = Dense(1000, activation='softmax')(x)\n",
        "\n",
        "# 새 모델 정의\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# 학습률 조정\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# 조기 종료 설정\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# 모델 학습\n",
        "model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=50,\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "# 모델 평가 및 예측은 동일\n",
        "val_loss, val_accuracy = model.evaluate(val_generator)\n",
        "print(f\"Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}\")\n",
        "\n",
        "# 예측 수행 (여기서는 샘플 이미지로 예시)\n",
        "y_pred = model.predict(image)\n",
        "decoded_predictions = decode_predictions(y_pred, top=3)[0]\n",
        "print(\"Predicted:\", decoded_predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSaTqAi6dbsY",
        "outputId": "c0cd9e49-971a-45ac-ab9b-d45de9eb54de"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 7s/step - accuracy: 0.3333 - loss: 4.4525 - val_accuracy: 1.0000 - val_loss: 5.7275e-05\n",
            "Epoch 2/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 134ms/step - accuracy: 1.0000 - loss: 0.0076 - val_accuracy: 1.0000 - val_loss: 5.9628e-05\n",
            "Epoch 3/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 249ms/step - accuracy: 1.0000 - loss: 5.4734e-06 - val_accuracy: 1.0000 - val_loss: 2.9802e-07\n",
            "Epoch 4/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 247ms/step - accuracy: 1.0000 - loss: 8.6923e-09 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 5/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 133ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 6/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 134ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 7/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 135ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 8/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 131ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 9/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 131ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
            "Validation Loss: 0.0, Validation Accuracy: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7f1638e9a0e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step\n",
            "Predicted: [('n01443537', 'goldfish', 1.0), ('n04392985', 'tape_player', 1.3625023e-14), ('n02219486', 'ant', 4.856669e-15)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "현재 모델이 매우 높은 정확도와 매우 낮은 손실을 기록하고 있는 것으로 보입니다. accuracy와 val_accuracy가 100%로 나타나고 있으며, loss와 val_loss 값이 거의 0에 수렴하는 상황입니다. 하지만, 이는 일반적으로 모델이 과적합(overfitting)되었거나, 학습 데이터셋이 지나치게 단순해서 발생하는 현상일 수 있습니다."
      ],
      "metadata": {
        "id": "7qq9UskbeHTI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# 모델의 복잡성 줄이기 및 드롭아웃 비율 증가\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "x = Dropout(0.6)(x)  # 드롭아웃 비율 증가\n",
        "predictions = Dense(1000, activation='softmax')(x)\n",
        "\n",
        "# 새 모델 정의\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# 학습률 조정\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# 데이터 증강 설정\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest',\n",
        "    validation_split=0.2  # 데이터의 20%를 검증 데이터로 사용\n",
        ")\n",
        "\n",
        "# 조기 종료 설정\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# 모델 학습\n",
        "model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=50,\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "# 모델 평가 및 예측은 동일\n",
        "val_loss, val_accuracy = model.evaluate(val_generator)\n",
        "print(f\"Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}\")\n",
        "\n",
        "# 예측 수행 (여기서는 샘플 이미지로 예시)\n",
        "y_pred = model.predict(image)\n",
        "decoded_predictions = decode_predictions(y_pred, top=3)[0]\n",
        "print(\"Predicted:\", decoded_predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LbSZRXTpeE98",
        "outputId": "6cf2d90d-52c2-4a62-d065-90c2d131ff1f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 4s/step - accuracy: 0.3333 - loss: 5.2474 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 2/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 138ms/step - accuracy: 1.0000 - loss: 0.0534 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 3/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 133ms/step - accuracy: 1.0000 - loss: 2.4835e-09 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 4/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 134ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 5/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 138ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 6/50\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 134ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
            "Validation Loss: 0.0, Validation Accuracy: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7f150be13d00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Predicted: [('n01443537', 'goldfish', 1.0), ('n02086646', 'Blenheim_spaniel', 1.1743297e-16), ('n07742313', 'Granny_Smith', 2.1711564e-17)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "과적합 방지 코드"
      ],
      "metadata": {
        "id": "Goz1sRsMgHLV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, BatchNormalization\n",
        "# from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# # 모델의 복잡성 줄이기 및 드롭아웃 비율 증가\n",
        "# x = base_model.output\n",
        "# x = GlobalAveragePooling2D()(x)\n",
        "# x = Dense(512, activation='relu')(x)\n",
        "# x = BatchNormalization()(x)  # 배치 정규화 추가\n",
        "# x = Dropout(0.7)(x)  # 드롭아웃 비율 증가\n",
        "# predictions = Dense(10, activation='softmax')(x)  # CIFAR-10 데이터셋의 클래스 수\n",
        "\n",
        "# # 모델 정의\n",
        "# model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# # 학습률 조정\n",
        "# model.compile(optimizer=Adam(learning_rate=0.00001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# # 데이터 증강 설정\n",
        "# datagen = ImageDataGenerator(\n",
        "#     rotation_range=40,\n",
        "#     width_shift_range=0.3,\n",
        "#     height_shift_range=0.3,\n",
        "#     shear_range=0.2,\n",
        "#     zoom_range=0.3,\n",
        "#     horizontal_flip=True,\n",
        "#     fill_mode='nearest',\n",
        "#     validation_split=0.2\n",
        "# )\n",
        "\n",
        "# # 데이터 증강 적용\n",
        "# train_generator = datagen.flow(x_train, y_train, batch_size=32)\n",
        "\n",
        "# # 조기 종료 설정\n",
        "# early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# # 모델 학습\n",
        "# model.fit(\n",
        "#     train_generator,\n",
        "#     epochs=50,\n",
        "#     validation_data=(x_test, y_test),\n",
        "#     callbacks=[early_stopping]\n",
        "# )\n",
        "\n",
        "# # 모델 평가 및 예측은 동일\n",
        "# val_loss, val_accuracy = model.evaluate(val_generator)\n",
        "# print(f\"Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}\")\n",
        "\n",
        "# # 예측 수행 (여기서는 샘플 이미지로 예시)\n",
        "# y_pred = model.predict(image)\n",
        "# decoded_predictions = decode_predictions(y_pred, top=3)[0]\n",
        "# print(\"Predicted:\", decoded_predictions)"
      ],
      "metadata": {
        "id": "NY1PAqfCe_mp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "다른 데이터셋으로 성능을 테스트하는 것은 모델이 다양한 상황에서 얼마나 잘 동작하는지 평가하는 중요한 과정입니다. TensorFlow/Keras는 다양한 공개 데이터셋을 제공합니다. 예를 들어, CIFAR-10, CIFAR-100, 또는 MNIST와 같은 데이터셋을 사용하여 모델의 성능을 테스트할 수 있습니다.\n",
        "\n",
        "아래는 CIFAR-10 데이터셋을 사용하여 InceptionV3 모델의 성능을 테스트하는 방법을 보여주는 예제입니다. CIFAR-10 데이터셋은 10개의 클래스로 구성된 6만 개의 32x32 크기의 컬러 이미지로 이루어져 있습니다.\n",
        "\n",
        "CIFAR-10 데이터셋을 사용한 성능 테스트 예제:"
      ],
      "metadata": {
        "id": "lXYekh9Tegcl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.applications import InceptionV3\n",
        "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# 1. CIFAR-10 데이터셋 로드 및 전처리\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# InceptionV3 모델에 맞게 이미지 크기를 75x75로 조정\n",
        "x_train = tf.image.resize(x_train, (75, 75))\n",
        "x_test = tf.image.resize(x_test, (75, 75))\n",
        "\n",
        "# 데이터 전처리\n",
        "x_train = preprocess_input(x_train)\n",
        "x_test = preprocess_input(x_test)\n",
        "\n",
        "# 레이블을 One-hot 인코딩\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# 2. InceptionV3 모델 로드 및 미세 조정 설정\n",
        "base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(75, 75, 3))\n",
        "\n",
        "# 3. 레이어 추가 및 모델 확장\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "predictions = Dense(10, activation='softmax')(x)  # CIFAR-10 데이터셋의 클래스 수\n",
        "\n",
        "# 모델 정의\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# 4. 일부 레이어 동결 해제 (Fine-tuning)\n",
        "for layer in base_model.layers[-50:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "# 5. 모델 컴파일\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# 6. 데이터 증강 설정\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "# 데이터 증강 적용\n",
        "train_generator = datagen.flow(x_train, y_train, batch_size=32)\n",
        "\n",
        "# 7. 조기 종료 설정\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# 8. 모델 학습\n",
        "model.fit(\n",
        "    train_generator,\n",
        "    epochs=50,\n",
        "    validation_data=(x_test, y_test),\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "# 9. 모델 평가\n",
        "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
        "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n",
        "\n",
        "# 10. 예측 수행\n",
        "y_pred = model.predict(x_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6XBCAITeg_e",
        "outputId": "303b05fe-573f-4ab0-a767-7ec5a7d3ecd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m87910968/87910968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 0us/step\n",
            "Epoch 1/50\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 80ms/step - accuracy: 0.4599 - loss: 1.5414 - val_accuracy: 0.8034 - val_loss: 0.5972\n",
            "Epoch 2/50\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 55ms/step - accuracy: 0.7589 - loss: 0.7155 - val_accuracy: 0.8470 - val_loss: 0.4502\n",
            "Epoch 3/50\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 55ms/step - accuracy: 0.8073 - loss: 0.5821 - val_accuracy: 0.8629 - val_loss: 0.4113\n",
            "Epoch 4/50\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 55ms/step - accuracy: 0.8330 - loss: 0.4961 - val_accuracy: 0.8854 - val_loss: 0.3498\n",
            "Epoch 5/50\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 55ms/step - accuracy: 0.8491 - loss: 0.4487 - val_accuracy: 0.8791 - val_loss: 0.5642\n",
            "Epoch 6/50\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 55ms/step - accuracy: 0.8627 - loss: 0.4096 - val_accuracy: 0.8875 - val_loss: 0.3382\n",
            "Epoch 7/50\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 55ms/step - accuracy: 0.8767 - loss: 0.3688 - val_accuracy: 0.8816 - val_loss: 0.4231\n",
            "Epoch 8/50\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 55ms/step - accuracy: 0.8879 - loss: 0.3375 - val_accuracy: 0.8935 - val_loss: 0.3169\n",
            "Epoch 9/50\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 55ms/step - accuracy: 0.8870 - loss: 0.3326 - val_accuracy: 0.9017 - val_loss: 0.2950\n",
            "Epoch 10/50\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 55ms/step - accuracy: 0.8955 - loss: 0.3163 - val_accuracy: 0.8998 - val_loss: 0.3029\n",
            "Epoch 11/50\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 55ms/step - accuracy: 0.9024 - loss: 0.2907 - val_accuracy: 0.9101 - val_loss: 0.2725\n",
            "Epoch 12/50\n",
            "\u001b[1m1205/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m19s\u001b[0m 54ms/step - accuracy: 0.9116 - loss: 0.2613"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "041tyTtKgLIx"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R0U_xk-RgLcl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}