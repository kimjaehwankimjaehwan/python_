{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimjaehwankimjaehwan/python_/blob/main/seyonec_PubChem10M_SMILES_BPE_450k.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ITCrlDVq9PSV",
        "outputId": "40d1e7a2-c5ae-4655-8acd-9065e6f8e06d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.0+cu121)\n",
            "Collecting rdkit\n",
            "  Downloading rdkit-2024.3.5-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit) (9.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Downloading rdkit-2024.3.5-cp310-cp310-manylinux_2_28_x86_64.whl (33.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m33.1/33.1 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rdkit\n",
            "Successfully installed rdkit-2024.3.5\n"
          ]
        }
      ],
      "source": [
        "pip install transformers torch rdkit pandas scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aBWuoil99SN7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        },
        "outputId": "28c92596-3424-4a78-b505-1363b313db6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at seyonec/PubChem10M_SMILES_BPE_450k and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='294' max='294' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [294/294 01:07, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Rmse</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.653618</td>\n",
              "      <td>0.808466</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.471863</td>\n",
              "      <td>0.686923</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.480935</td>\n",
              "      <td>0.693495</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation RMSE: 0.6869229707557296\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# ÏÑ§Ï†ï\n",
        "CFG = {\n",
        "    'SEED': 42,\n",
        "    'MODEL_NAME': 'seyonec/PubChem10M_SMILES_BPE_450k',\n",
        "    'BATCH_SIZE': 16,\n",
        "    'EPOCHS': 3,\n",
        "    'LR': 5e-5,\n",
        "}\n",
        "\n",
        "def seed_everything(seed):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "seed_everything(CFG['SEED'])\n",
        "\n",
        "# Îç∞Ïù¥ÌÑ∞ Î°úÎìú\n",
        "chembl_data = pd.read_csv('train.csv')  # ÏòàÏãú ÌååÏùº Ïù¥Î¶Ñ\n",
        "train, val = train_test_split(chembl_data, test_size=0.2, random_state=CFG['SEED'])\n",
        "\n",
        "# ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä Î∞è Î™®Îç∏ Î°úÎìú\n",
        "tokenizer = AutoTokenizer.from_pretrained(CFG['MODEL_NAME'])\n",
        "model = AutoModelForSequenceClassification.from_pretrained(CFG['MODEL_NAME'], num_labels=1)\n",
        "\n",
        "# Îç∞Ïù¥ÌÑ∞ÏÖã Ï†ïÏùò\n",
        "class SMILESDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data, tokenizer, max_len=128, has_target=True): # Added has_target parameter\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.has_target = has_target # Store has_target value\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        smiles = self.data.iloc[index]['Smiles']\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            smiles,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_len,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        if self.has_target: # Check if target should be included\n",
        "            target = self.data.iloc[index]['pIC50']\n",
        "            return {\n",
        "                'input_ids': encoding['input_ids'].squeeze(),\n",
        "                'attention_mask': encoding['attention_mask'].squeeze(),\n",
        "                'labels': torch.tensor(target, dtype=torch.float)\n",
        "            }\n",
        "        else: # Return only input_ids and attention_mask\n",
        "            return {\n",
        "                'input_ids': encoding['input_ids'].squeeze(),\n",
        "                'attention_mask': encoding['attention_mask'].squeeze(),\n",
        "            }\n",
        "\n",
        "train_dataset = SMILESDataset(train, tokenizer)\n",
        "val_dataset = SMILESDataset(val, tokenizer)\n",
        "\n",
        "# TrainingArguments Î∞è Trainer ÏÑ§Ï†ï\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    evaluation_strategy='epoch', #Evaluation strategy matches save strategy\n",
        "    learning_rate=CFG['LR'],\n",
        "    per_device_train_batch_size=CFG['BATCH_SIZE'],\n",
        "    per_device_eval_batch_size=CFG['BATCH_SIZE'],\n",
        "    num_train_epochs=CFG['EPOCHS'],\n",
        "    seed=CFG['SEED'],\n",
        "    load_best_model_at_end=True,\n",
        "    logging_dir='./logs',\n",
        "    save_strategy = 'epoch' # Changed to epoch to match evaluation strategy\n",
        "\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=lambda p: {'rmse': np.sqrt(mean_squared_error(p.label_ids, p.predictions.flatten()))}\n",
        ")\n",
        "\n",
        "# Î™®Îç∏ ÌïôÏäµ\n",
        "trainer.train()\n",
        "\n",
        "# Í≤ÄÏ¶ù Îç∞Ïù¥ÌÑ∞ ÏòàÏ∏° Î∞è ÌèâÍ∞Ä\n",
        "val_preds = trainer.predict(val_dataset)\n",
        "val_rmse = np.sqrt(mean_squared_error(val['pIC50'], val_preds.predictions.flatten()))\n",
        "print(f'Validation RMSE: {val_rmse}')\n",
        "\n",
        "# ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞ ÏòàÏ∏°\n",
        "test = pd.read_csv('./test.csv')\n",
        "test_dataset = SMILESDataset(test, tokenizer, has_target=False) # Set has_target to False for the test dataset\n",
        "test_preds = trainer.predict(test_dataset)\n",
        "\n",
        "# pIC50ÏùÑ IC50ÏúºÎ°ú Î≥ÄÌôò\n",
        "def pIC50_to_IC50(pic50_values):\n",
        "    return 10 ** (9 - pic50_values)\n",
        "\n",
        "test['IC50_nM'] = pIC50_to_IC50(test_preds.predictions.flatten())\n",
        "\n",
        "# Ï†úÏ∂ú ÌååÏùº Ï†ÄÏû•\n",
        "submit = pd.read_csv('./sample_submission.csv')\n",
        "submit['IC50_nM'] = test['IC50_nM']\n",
        "submit.to_csv('./transformer_baseline_submit.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s0t2vd659SRY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "outputId": "76a141ac-b1cc-4355-dd25-bc3462ba0231"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at seyonec/PubChem10M_SMILES_BPE_450k and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='490' max='490' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [490/490 01:53, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Rmse</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.698729</td>\n",
              "      <td>0.835900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.521701</td>\n",
              "      <td>0.722289</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.624996</td>\n",
              "      <td>0.790567</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.534792</td>\n",
              "      <td>0.731295</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.556339</td>\n",
              "      <td>0.745881</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation RMSE: 0.7222886035452358\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, get_linear_schedule_with_warmup\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# ÏÑ§Ï†ï\n",
        "CFG = {\n",
        "    'SEED': 42,\n",
        "    'MODEL_NAME': 'seyonec/PubChem10M_SMILES_BPE_450k',\n",
        "    'BATCH_SIZE': 8,  # Batch size reduced for finer gradients\n",
        "    'EPOCHS': 5,  # Increased number of epochs\n",
        "    'LR': 2e-5,  # Reduced learning rate for more stable training\n",
        "    'WARMUP_RATIO': 0.1,  # Warmup ratio for the learning rate scheduler\n",
        "}\n",
        "\n",
        "def seed_everything(seed):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "seed_everything(CFG['SEED'])\n",
        "\n",
        "# Îç∞Ïù¥ÌÑ∞ Î°úÎìú\n",
        "chembl_data = pd.read_csv('train.csv')  # ÏòàÏãú ÌååÏùº Ïù¥Î¶Ñ\n",
        "train, val = train_test_split(chembl_data, test_size=0.2, random_state=CFG['SEED'])\n",
        "\n",
        "# ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä Î∞è Î™®Îç∏ Î°úÎìú\n",
        "tokenizer = AutoTokenizer.from_pretrained(CFG['MODEL_NAME'])\n",
        "model = AutoModelForSequenceClassification.from_pretrained(CFG['MODEL_NAME'], num_labels=1)\n",
        "\n",
        "# Îç∞Ïù¥ÌÑ∞ÏÖã Ï†ïÏùò\n",
        "class SMILESDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data, tokenizer, max_len=128, has_target=True):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.has_target = has_target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        smiles = self.data.iloc[index]['Smiles']\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            smiles,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_len,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        if self.has_target:\n",
        "            target = self.data.iloc[index]['pIC50']\n",
        "            return {\n",
        "                'input_ids': encoding['input_ids'].squeeze(),\n",
        "                'attention_mask': encoding['attention_mask'].squeeze(),\n",
        "                'labels': torch.tensor(target, dtype=torch.float)\n",
        "            }\n",
        "        else:\n",
        "            return {\n",
        "                'input_ids': encoding['input_ids'].squeeze(),\n",
        "                'attention_mask': encoding['attention_mask'].squeeze(),\n",
        "            }\n",
        "\n",
        "train_dataset = SMILESDataset(train, tokenizer)\n",
        "val_dataset = SMILESDataset(val, tokenizer)\n",
        "\n",
        "# ÏòµÌã∞ÎßàÏù¥Ï†Ä Î∞è Ïä§ÏºÄÏ§ÑÎü¨ ÏÑ§Ï†ï\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=CFG['LR'])\n",
        "total_steps = len(train_dataset) // CFG['BATCH_SIZE'] * CFG['EPOCHS']\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=int(CFG['WARMUP_RATIO'] * total_steps),\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "# TrainingArguments Î∞è Trainer ÏÑ§Ï†ï\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    evaluation_strategy='epoch',\n",
        "    learning_rate=CFG['LR'],\n",
        "    per_device_train_batch_size=CFG['BATCH_SIZE'],\n",
        "    per_device_eval_batch_size=CFG['BATCH_SIZE'],\n",
        "    num_train_epochs=CFG['EPOCHS'],\n",
        "    seed=CFG['SEED'],\n",
        "    load_best_model_at_end=True,\n",
        "    logging_dir='./logs',\n",
        "    save_strategy='epoch',\n",
        "    gradient_accumulation_steps=2,  # To simulate a larger batch size\n",
        "    #optimizers=(optimizer, scheduler)  # Custom optimizer and scheduler - This line is removed\n",
        ")\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=lambda p: {'rmse': np.sqrt(mean_squared_error(p.label_ids, p.predictions.flatten()))}\n",
        ")\n",
        "\n",
        "# Î™®Îç∏ ÌïôÏäµ\n",
        "trainer.train()\n",
        "\n",
        "# Í≤ÄÏ¶ù Îç∞Ïù¥ÌÑ∞ ÏòàÏ∏° Î∞è ÌèâÍ∞Ä\n",
        "val_preds = trainer.predict(val_dataset)\n",
        "val_rmse = np.sqrt(mean_squared_error(val['pIC50'], val_preds.predictions.flatten()))\n",
        "print(f'Validation RMSE: {val_rmse}')\n",
        "\n",
        "# ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞ ÏòàÏ∏°\n",
        "test = pd.read_csv('./test.csv')\n",
        "test_dataset = SMILESDataset(test, tokenizer, has_target=False)\n",
        "test_preds = trainer.predict(test_dataset)\n",
        "\n",
        "# pIC50ÏùÑ IC50ÏúºÎ°ú Î≥ÄÌôò\n",
        "def pIC50_to_IC50(pic50_values):\n",
        "    return 10 ** (9 - pic50_values)\n",
        "\n",
        "test['IC50_nM'] = pIC50_to_IC50(test_preds.predictions.flatten())\n",
        "\n",
        "# Ï†úÏ∂ú ÌååÏùº Ï†ÄÏû•\n",
        "submit = pd.read_csv('./sample_submission.csv')\n",
        "submit['IC50_nM'] = test['IC50_nM']\n",
        "submit.to_csv('./transformer_tuned_submit.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. ÌïôÏäµÎ•†(LR): Í∏∞Î≥∏ ÌïôÏäµÎ•†ÏùÑ 2e-5Î°ú Ï§ÑÏòÄÏäµÎãàÎã§. Îçî ÎÇÆÏùÄ ÌïôÏäµÎ•†ÏùÄ ÌïôÏäµÏùò ÏïàÏ†ïÏÑ±ÏùÑ ÎÜíÏó¨Ï§Ñ Ïàò ÏûàÏäµÎãàÎã§.\n",
        "2. ÏóêÌè¨ÌÅ¨ Ïàò Ï¶ùÍ∞Ä: ÏóêÌè¨ÌÅ¨ ÏàòÎ•º 5Î°ú ÎäòÎ†§ÏÑú Î™®Îç∏Ïù¥ Ï∂©Î∂ÑÌûà ÌïôÏäµÌï† Ïàò ÏûàÍ≤å ÌñàÏäµÎãàÎã§.\n",
        "3. Î∞∞Ïπò ÌÅ¨Í∏∞ Í∞êÏÜå: Î∞∞Ïπò ÌÅ¨Í∏∞Î•º 8Î°ú Ï§ÑÏó¨ÏÑú Î©îÎ™®Î¶¨ ÏÇ¨Ïö©ÏùÑ Ï§ÑÏù¥Í≥†, ÏÑ∏Î∞ÄÌïú ÌïôÏäµÏù¥ Í∞ÄÎä•ÌïòÎèÑÎ°ù ÌñàÏäµÎãàÎã§.\n",
        "4. Warmup Îã®Í≥Ñ Ï∂îÍ∞Ä: WARMUP_RATIOÎ•º ÏÇ¨Ïö©ÌïòÏó¨ Ï¥àÍ∏∞ Î™á Îã®Í≥Ñ ÎèôÏïà ÌïôÏäµÎ•†ÏùÑ Ï†êÏßÑÏ†ÅÏúºÎ°ú Ï¶ùÍ∞ÄÏãúÌÇ§Îäî warmup Îã®Í≥ÑÎ•º Ï∂îÍ∞ÄÌñàÏäµÎãàÎã§.\n",
        "5. Gradient Accumulation: ÏûëÏùÄ Î∞∞Ïπò ÌÅ¨Í∏∞Î•º ÏÇ¨Ïö©ÌïòÎäî ÎåÄÏã†, 6. gradient_accumulation_stepsÎ•º 2Î°ú ÏÑ§Ï†ïÌïòÏó¨ Ïã§ÏßàÏ†ÅÏúºÎ°ú Î∞∞Ïπò ÌÅ¨Í∏∞Î•º 16ÏúºÎ°ú ÏãúÎÆ¨Î†àÏù¥ÏÖòÌñàÏäµÎãàÎã§.\n",
        "6. Custom Optimizer and Scheduler: ÏòµÌã∞ÎßàÏù¥Ï†ÄÏôÄ ÌïôÏäµÎ•† Ïä§ÏºÄÏ§ÑÎü¨Î•º ÏßÅÏ†ë ÏÑ§Ï†ïÌïòÏó¨ Îçî Ï†ïÎ∞ÄÌïòÍ≤å ÌïôÏäµÏùÑ Ï†úÏñ¥Ìï† Ïàò ÏûàÎèÑÎ°ù ÌñàÏäµÎãàÎã§."
      ],
      "metadata": {
        "id": "eDjdJ6APAbv4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HtOASWNrNCRc",
        "outputId": "a26ae0d5-dbd1-46cc-fac3-b1a800d67030"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Sep  1 15:31:07 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P0              46W / 400W |      2MiB / 40960MiB |      0%      Default |\n",
            "|                                         |                      |             Disabled |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "metadata": {
        "id": "xEa8O7hONGy3",
        "outputId": "2ab2b803-8669-4506-bacb-accfd4940306",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 89.6 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jTm-qBFq9Sav",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "efa94a72-a92d-4030-cb06-a2c22fecde4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training fold 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at seyonec/PubChem10M_SMILES_BPE_450k and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='980' max='980' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [980/980 01:42, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Rmse</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>31.124796</td>\n",
              "      <td>5.578960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.220578</td>\n",
              "      <td>1.104797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.844613</td>\n",
              "      <td>0.919029</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.696253</td>\n",
              "      <td>0.834418</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.558804</td>\n",
              "      <td>0.747532</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>13.039800</td>\n",
              "      <td>0.887116</td>\n",
              "      <td>0.941868</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>13.039800</td>\n",
              "      <td>0.633695</td>\n",
              "      <td>0.796049</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>13.039800</td>\n",
              "      <td>0.548072</td>\n",
              "      <td>0.740319</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>13.039800</td>\n",
              "      <td>0.690251</td>\n",
              "      <td>0.830814</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>13.039800</td>\n",
              "      <td>0.739088</td>\n",
              "      <td>0.859702</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training fold 2/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at seyonec/PubChem10M_SMILES_BPE_450k and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='980' max='980' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [980/980 01:43, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Rmse</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>29.958324</td>\n",
              "      <td>5.473420</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.061877</td>\n",
              "      <td>1.030474</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.830433</td>\n",
              "      <td>0.911281</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.718589</td>\n",
              "      <td>0.847696</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.738732</td>\n",
              "      <td>0.859495</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>12.529500</td>\n",
              "      <td>0.832161</td>\n",
              "      <td>0.912229</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>12.529500</td>\n",
              "      <td>0.595247</td>\n",
              "      <td>0.771523</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>12.529500</td>\n",
              "      <td>0.680827</td>\n",
              "      <td>0.825123</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>12.529500</td>\n",
              "      <td>0.580379</td>\n",
              "      <td>0.761826</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>12.529500</td>\n",
              "      <td>0.648351</td>\n",
              "      <td>0.805203</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training fold 3/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at seyonec/PubChem10M_SMILES_BPE_450k and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='980' max='980' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [980/980 01:41, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Rmse</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>31.337585</td>\n",
              "      <td>5.597998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.089720</td>\n",
              "      <td>1.043897</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.575194</td>\n",
              "      <td>0.758415</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.831460</td>\n",
              "      <td>0.911844</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.650155</td>\n",
              "      <td>0.806322</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>12.419100</td>\n",
              "      <td>0.860149</td>\n",
              "      <td>0.927442</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>12.419100</td>\n",
              "      <td>0.618002</td>\n",
              "      <td>0.786131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>12.419100</td>\n",
              "      <td>0.613324</td>\n",
              "      <td>0.783150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>12.419100</td>\n",
              "      <td>0.689739</td>\n",
              "      <td>0.830505</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>12.419100</td>\n",
              "      <td>0.566260</td>\n",
              "      <td>0.752503</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training fold 4/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at seyonec/PubChem10M_SMILES_BPE_450k and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='980' max='980' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [980/980 01:41, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Rmse</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>30.598385</td>\n",
              "      <td>5.531581</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.953274</td>\n",
              "      <td>0.976358</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.544108</td>\n",
              "      <td>0.737637</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.847588</td>\n",
              "      <td>0.920645</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.591387</td>\n",
              "      <td>0.769017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>12.488600</td>\n",
              "      <td>0.851984</td>\n",
              "      <td>0.923030</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>12.488600</td>\n",
              "      <td>0.652355</td>\n",
              "      <td>0.807685</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>12.488600</td>\n",
              "      <td>0.662631</td>\n",
              "      <td>0.814021</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>12.488600</td>\n",
              "      <td>0.593564</td>\n",
              "      <td>0.770431</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>12.488600</td>\n",
              "      <td>0.593447</td>\n",
              "      <td>0.770355</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training fold 5/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at seyonec/PubChem10M_SMILES_BPE_450k and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='980' max='980' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [980/980 01:44, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Rmse</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>29.266417</td>\n",
              "      <td>5.409844</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.034485</td>\n",
              "      <td>1.017096</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.736941</td>\n",
              "      <td>0.858453</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.788320</td>\n",
              "      <td>0.887874</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.763934</td>\n",
              "      <td>0.874033</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>12.568500</td>\n",
              "      <td>0.769360</td>\n",
              "      <td>0.877132</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>12.568500</td>\n",
              "      <td>0.813334</td>\n",
              "      <td>0.901850</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>12.568500</td>\n",
              "      <td>0.856134</td>\n",
              "      <td>0.925275</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>12.568500</td>\n",
              "      <td>0.720677</td>\n",
              "      <td>0.848927</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>12.568500</td>\n",
              "      <td>0.797771</td>\n",
              "      <td>0.893180</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ensemble Validation RMSE: 0.5112974297320854\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, get_cosine_schedule_with_warmup\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from transformers import AdamW\n",
        "\n",
        "# ÏÑ§Ï†ï\n",
        "CFG = {\n",
        "    'SEED': 42,\n",
        "    'MODEL_NAME': 'seyonec/PubChem10M_SMILES_BPE_450k',\n",
        "    'BATCH_SIZE': 8,\n",
        "    'EPOCHS': 10,  # Increased number of epochs\n",
        "    'LR': 1e-5,  # Reduced learning rate\n",
        "    'WARMUP_RATIO': 0.2,  # Increased warmup ratio\n",
        "    'ENSEMBLE_MODELS': 3,  # Number of models in the ensemble\n",
        "    'K_FOLDS': 5  # Number of folds for cross-validation\n",
        "}\n",
        "\n",
        "def seed_everything(seed):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "seed_everything(CFG['SEED'])\n",
        "\n",
        "# Îç∞Ïù¥ÌÑ∞ Î°úÎìú\n",
        "chembl_data = pd.read_csv('train.csv')  # ÏòàÏãú ÌååÏùº Ïù¥Î¶Ñ\n",
        "\n",
        "# ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä Î∞è Î™®Îç∏ Î°úÎìú\n",
        "tokenizer = AutoTokenizer.from_pretrained(CFG['MODEL_NAME'])\n",
        "\n",
        "# Îç∞Ïù¥ÌÑ∞ÏÖã Ï†ïÏùò\n",
        "class SMILESDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data, tokenizer, max_len=128, has_target=True):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.has_target = has_target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        smiles = self.data.iloc[index]['Smiles']\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            smiles,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_len,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        if self.has_target:\n",
        "            target = self.data.iloc[index]['pIC50']\n",
        "            return {\n",
        "                'input_ids': encoding['input_ids'].squeeze(),\n",
        "                'attention_mask': encoding['attention_mask'].squeeze(),\n",
        "                'labels': torch.tensor(target, dtype=torch.float)\n",
        "            }\n",
        "        else:\n",
        "            return {\n",
        "                'input_ids': encoding['input_ids'].squeeze(),\n",
        "                'attention_mask': encoding['attention_mask'].squeeze(),\n",
        "            }\n",
        "\n",
        "def create_optimizer_and_scheduler(model, total_steps):\n",
        "    optimizer = AdamW(model.parameters(), lr=CFG['LR'])\n",
        "    scheduler = get_cosine_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=int(CFG['WARMUP_RATIO'] * total_steps),\n",
        "        num_training_steps=total_steps\n",
        "    )\n",
        "    return optimizer, scheduler\n",
        "\n",
        "def train_and_evaluate(train_df, val_df, fold_idx):\n",
        "    train_dataset = SMILESDataset(train_df, tokenizer)\n",
        "    val_dataset = SMILESDataset(val_df, tokenizer)\n",
        "\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(CFG['MODEL_NAME'], num_labels=1)\n",
        "\n",
        "    total_steps = len(train_dataset) // CFG['BATCH_SIZE'] * CFG['EPOCHS']\n",
        "    optimizer, scheduler = create_optimizer_and_scheduler(model, total_steps)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f'./results_fold_{fold_idx}',\n",
        "        evaluation_strategy='epoch',\n",
        "        per_device_train_batch_size=CFG['BATCH_SIZE'],\n",
        "        per_device_eval_batch_size=CFG['BATCH_SIZE'],\n",
        "        num_train_epochs=CFG['EPOCHS'],\n",
        "        seed=CFG['SEED'],\n",
        "        load_best_model_at_end=True,\n",
        "        logging_dir=f'./logs_fold_{fold_idx}',\n",
        "        save_strategy='epoch',\n",
        "        gradient_accumulation_steps=2,  # To simulate a larger batch size\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        compute_metrics=lambda p: {'rmse': np.sqrt(mean_squared_error(p.label_ids, p.predictions.flatten()))},\n",
        "        optimizers=(optimizer, scheduler)\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    # Î™®Îç∏ Ï†ÄÏû•\n",
        "    trainer.save_model(f'./results_fold_{fold_idx}')\n",
        "\n",
        "    # Í≤ÄÏ¶ù Îç∞Ïù¥ÌÑ∞ ÏòàÏ∏°\n",
        "    val_preds = trainer.predict(val_dataset)\n",
        "    return val_preds.predictions.flatten()\n",
        "\n",
        "# Cross-Validation Í∏∞Î∞ò ÏïôÏÉÅÎ∏î\n",
        "kf = KFold(n_splits=CFG['K_FOLDS'], shuffle=True, random_state=CFG['SEED'])\n",
        "ensemble_preds = []\n",
        "\n",
        "for fold_idx, (train_index, val_index) in enumerate(kf.split(chembl_data)):\n",
        "    print(f\"Training fold {fold_idx+1}/{CFG['K_FOLDS']}\")\n",
        "    train_df, val_df = chembl_data.iloc[train_index], chembl_data.iloc[val_index]\n",
        "    fold_preds = train_and_evaluate(train_df, val_df, fold_idx)\n",
        "    ensemble_preds.append(fold_preds)\n",
        "\n",
        "# Í∞Å Ìè¥ÎìúÏùò ÏòàÏ∏° Í∞íÏùÑ numpy Î∞∞Ïó¥Î°ú Î≥ÄÌôòÌïòÍ≥†, ÌÅ¨Í∏∞Î•º ÎßûÏ∂∞Ï§å\n",
        "ensemble_preds = [np.array(preds) for preds in ensemble_preds]\n",
        "min_length = min([len(preds) for preds in ensemble_preds])\n",
        "ensemble_preds = [preds[:min_length] for preds in ensemble_preds]\n",
        "\n",
        "# ÏïôÏÉÅÎ∏î Í≤∞Í≥º Í≥ÑÏÇ∞\n",
        "final_ensemble_preds = np.mean(ensemble_preds, axis=0)\n",
        "\n",
        "# Ïò¨Î∞îÎ•¥Í≤å Ïù∏Îç±Ïä§Î•º ÏÇ¨Ïö©ÌïòÏó¨ RMSE Í≥ÑÏÇ∞\n",
        "val_index = np.concatenate([val_index for _, val_index in kf.split(chembl_data)])\n",
        "val_rmse = np.sqrt(mean_squared_error(chembl_data.iloc[val_index[:min_length]]['pIC50'], final_ensemble_preds))\n",
        "print(f'Ensemble Validation RMSE: {val_rmse}')\n",
        "\n",
        "# ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞ ÏòàÏ∏°\n",
        "test = pd.read_csv('./test.csv')\n",
        "test_dataset = SMILESDataset(test, tokenizer, has_target=False)\n",
        "\n",
        "ensemble_test_preds = []\n",
        "\n",
        "for fold_idx in range(CFG['K_FOLDS']):\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(f'./results_fold_{fold_idx}', num_labels=1)\n",
        "    trainer = Trainer(model=model)\n",
        "    test_preds = trainer.predict(test_dataset)\n",
        "    ensemble_test_preds.append(test_preds.predictions.flatten())\n",
        "\n",
        "# ÏµúÏ¢Ö ÏïôÏÉÅÎ∏î ÌÖåÏä§Ìä∏ ÏòàÏ∏°\n",
        "final_test_preds = np.mean(ensemble_test_preds, axis=0)\n",
        "\n",
        "# pIC50ÏùÑ IC50ÏúºÎ°ú Î≥ÄÌôò\n",
        "def pIC50_to_IC50(pic50_values):\n",
        "    return 10 ** (9 - pic50_values)\n",
        "\n",
        "test['IC50_nM'] = pIC50_to_IC50(final_test_preds)\n",
        "\n",
        "# Ï†úÏ∂ú ÌååÏùº Ï†ÄÏû•\n",
        "submit = pd.read_csv('./sample_submission.csv')\n",
        "submit['IC50_nM'] = test['IC50_nM']\n",
        "submit.to_csv('./transformer_ensemble_submit.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Cross-ValidationÏùÑ Ïù¥Ïö©Ìïú ÏïôÏÉÅÎ∏î:\n",
        "Ïó¨Îü¨ Î™®Îç∏ÏùÑ ÌïôÏäµÌïòÎäî ÎåÄÏã† ÍµêÏ∞® Í≤ÄÏ¶ù(Cross-Validation) Í∏∞Î∞òÏúºÎ°ú Ïó¨Îü¨ Î∂ÑÌï†ÏóêÏÑú Î™®Îç∏ÏùÑ ÌõàÎ†®ÌïòÍ≥† Í∑∏ Í≤∞Í≥ºÎ•º ÏïôÏÉÅÎ∏îÌï† Ïàò ÏûàÏäµÎãàÎã§. Ïù¥ Î∞©Î≤ïÏùÄ Î™®Îç∏Ïùò ÏùºÎ∞òÌôî ÏÑ±Îä•ÏùÑ ÎÜíÏùº Ïàò ÏûàÏäµÎãàÎã§.\n",
        "\n",
        "2. Learning RateÎ•º Îã®Í≥ÑÏ†ÅÏúºÎ°ú Ï°∞Ï†à:\n",
        "Learning Rate SchedulerÎ•º Ï°∞Ï†ïÌïòÏó¨ ÌïôÏäµ Ï§ëÌõÑÎ∞òÎ∂ÄÏóê Ï¢Ä Îçî ÏûëÏùÄ Learning RateÎ•º ÏÇ¨Ïö©Ìï† Ïàò ÏûàÏäµÎãàÎã§. get_linear_schedule_with_warmup Í∞ôÏùÄ Î∞©Î≤ï ÎåÄÏã† get_cosine_schedule_with_warmupÏùÑ ÏÇ¨Ïö©ÌïòÎäî Í≤ÉÏù¥ Ìïú Í∞ÄÏßÄ Î∞©Î≤ïÏûÖÎãàÎã§.\n",
        "\n",
        "3. Data Augmentation:\n",
        "SMILES ÌëúÌòÑÏùÑ ÎûúÎç§ÌïòÍ≤å Îí§ÏÑûÎäî Îì±Ïùò Î∞©Î≤ïÏúºÎ°ú Îç∞Ïù¥ÌÑ∞Î•º Ï¶ùÍ∞ïÌï† Ïàò ÏûàÏäµÎãàÎã§. Ïù¥Î†áÍ≤å ÌïòÎ©¥ Î™®Îç∏Ïù¥ Îã§ÏñëÌïú ÏûÖÎ†•Ïóê ÎåÄÌï¥ Ï¢Ä Îçî Í∞ïÍ±¥ÌïòÍ≤å ÌïôÏäµÎê† Ïàò ÏûàÏäµÎãàÎã§.\n",
        "\n",
        "4. Hyperparameter Tuning:\n",
        "HyperparameterÎ•º ÏµúÏ†ÅÌôîÌïòÎäî Î∞©Î≤ïÏúºÎ°ú optunaÏôÄ Í∞ôÏùÄ ÎùºÏù¥Î∏åÎü¨Î¶¨Î•º ÏÇ¨Ïö©Ìï† Ïàò ÏûàÏäµÎãàÎã§. Îã§ÏñëÌïú ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ Ï°∞Ìï©ÏùÑ ÏûêÎèôÏúºÎ°ú ÌÉêÏÉâÌïòÏó¨ ÏµúÏ†ÅÏùò ÌååÎùºÎØ∏ÌÑ∞Î•º Ï∞æÏùÑ Ïàò ÏûàÏäµÎãàÎã§.\n",
        "\n",
        "5. Feature Engineering:\n",
        "SMILES Ïô∏Ïóê Î∂ÑÏûê Íµ¨Ï°∞Ïóê ÎåÄÌïú Ï∂îÍ∞ÄÏ†ÅÏù∏ ÌôîÌïôÏ†Å Ï†ïÎ≥¥Î•º Î™®Îç∏Ïóê Ìè¨Ìï®ÏãúÏºú ÏÑ±Îä•ÏùÑ Ìñ•ÏÉÅÏãúÌÇ¨ Ïàò ÏûàÏäµÎãàÎã§. ÏòàÎ•º Îì§Ïñ¥, Î∂ÑÏûêÎüâ, Í∑πÏÑ± ÌëúÎ©¥Ï†Å Îì±Ïùò Î¨ºÎ¶¨ÌôîÌïôÏ†Å ÌäπÏÑ±ÏùÑ Ìè¨Ìï®ÌïòÎäî Î∞©Î≤ïÏù¥ ÏûàÏäµÎãàÎã§.\n",
        "\n",
        "6. Dropout Î∞è Regularization:\n",
        "Î™®Îç∏Ïóê DropoutÏùÑ Ï∂îÍ∞ÄÌïòÍ±∞ÎÇò Í∞ÄÏ§ëÏπò Í∞êÏá†(Weight Decay)ÏôÄ Í∞ôÏùÄ Ï†ïÍ∑úÌôîÎ•º Îçî Í∞ïÌïòÍ≤å ÏÑ§Ï†ïÌï† Ïàò ÏûàÏäµÎãàÎã§."
      ],
      "metadata": {
        "id": "4SgX05fIOaHW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, get_cosine_schedule_with_warmup\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from transformers import AdamW\n",
        "\n",
        "# ÏÑ§Ï†ï\n",
        "CFG = {\n",
        "    'SEED': 42,\n",
        "    'MODEL_NAME': 'seyonec/PubChem10M_SMILES_BPE_450k',\n",
        "    'BATCH_SIZE': 8,\n",
        "    'EPOCHS': 10,  # Increased number of epochs\n",
        "    'LR': 1e-5,  # Reduced learning rate\n",
        "    'WARMUP_RATIO': 0.2,  # Increased warmup ratio\n",
        "    'ENSEMBLE_MODELS': 3,  # Number of models in the ensemble\n",
        "    'K_FOLDS': 5  # Number of folds for cross-validation\n",
        "}\n",
        "\n",
        "def seed_everything(seed):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "seed_everything(CFG['SEED'])\n",
        "\n",
        "# Îç∞Ïù¥ÌÑ∞ Î°úÎìú\n",
        "chembl_data = pd.read_csv('train.csv')  # ÏòàÏãú ÌååÏùº Ïù¥Î¶Ñ\n",
        "\n",
        "# ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä Î∞è Î™®Îç∏ Î°úÎìú\n",
        "tokenizer = AutoTokenizer.from_pretrained(CFG['MODEL_NAME'])\n",
        "\n",
        "# Îç∞Ïù¥ÌÑ∞ÏÖã Ï†ïÏùò\n",
        "class SMILESDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data, tokenizer, max_len=128, has_target=True):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.has_target = has_target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        smiles = self.data.iloc[index]['Smiles']\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            smiles,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_len,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        if self.has_target:\n",
        "            target = self.data.iloc[index]['pIC50']\n",
        "            return {\n",
        "                'input_ids': encoding['input_ids'].squeeze(),\n",
        "                'attention_mask': encoding['attention_mask'].squeeze(),\n",
        "                'labels': torch.tensor(target, dtype=torch.float)\n",
        "            }\n",
        "        else:\n",
        "            return {\n",
        "                'input_ids': encoding['input_ids'].squeeze(),\n",
        "                'attention_mask': encoding['attention_mask'].squeeze(),\n",
        "            }\n",
        "\n",
        "def create_optimizer_and_scheduler(model, total_steps):\n",
        "    optimizer = AdamW(model.parameters(), lr=CFG['LR'])\n",
        "    scheduler = get_cosine_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=int(CFG['WARMUP_RATIO'] * total_steps),\n",
        "        num_training_steps=total_steps\n",
        "    )\n",
        "    return optimizer, scheduler\n",
        "\n",
        "def train_and_evaluate(train_df, val_df, fold_idx):\n",
        "    train_dataset = SMILESDataset(train_df, tokenizer)\n",
        "    val_dataset = SMILESDataset(val_df, tokenizer)\n",
        "\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(CFG['MODEL_NAME'], num_labels=1)\n",
        "\n",
        "    total_steps = len(train_dataset) // CFG['BATCH_SIZE'] * CFG['EPOCHS']\n",
        "    optimizer, scheduler = create_optimizer_and_scheduler(model, total_steps)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f'./results_fold_{fold_idx}',\n",
        "        evaluation_strategy='epoch',\n",
        "        per_device_train_batch_size=CFG['BATCH_SIZE'],\n",
        "        per_device_eval_batch_size=CFG['BATCH_SIZE'],\n",
        "        num_train_epochs=CFG['EPOCHS'],\n",
        "        seed=CFG['SEED'],\n",
        "        load_best_model_at_end=True,\n",
        "        logging_dir=f'./logs_fold_{fold_idx}',\n",
        "        save_strategy='epoch',\n",
        "        gradient_accumulation_steps=2,  # To simulate a larger batch size\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        compute_metrics=lambda p: {'rmse': np.sqrt(mean_squared_error(p.label_ids, p.predictions.flatten()))},\n",
        "        optimizers=(optimizer, scheduler)\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    # Î™®Îç∏ Ï†ÄÏû•\n",
        "    trainer.save_model(f'./results_fold_{fold_idx}')\n",
        "\n",
        "    # Í≤ÄÏ¶ù Îç∞Ïù¥ÌÑ∞ ÏòàÏ∏°\n",
        "    val_preds = trainer.predict(val_dataset)\n",
        "    return val_preds.predictions.flatten()\n",
        "\n",
        "# Cross-Validation Í∏∞Î∞ò ÏïôÏÉÅÎ∏î\n",
        "kf = KFold(n_splits=CFG['K_FOLDS'], shuffle=True, random_state=CFG['SEED'])\n",
        "ensemble_preds = []\n",
        "\n",
        "for fold_idx, (train_index, val_index) in enumerate(kf.split(chembl_data)):\n",
        "    print(f\"Training fold {fold_idx+1}/{CFG['K_FOLDS']}\")\n",
        "    train_df, val_df = chembl_data.iloc[train_index], chembl_data.iloc[val_index]\n",
        "    fold_preds = train_and_evaluate(train_df, val_df, fold_idx)\n",
        "    ensemble_preds.append(fold_preds)\n",
        "\n",
        "# Í∞Å Ìè¥ÎìúÏùò ÏòàÏ∏° Í∞íÏùÑ numpy Î∞∞Ïó¥Î°ú Î≥ÄÌôòÌïòÍ≥†, ÌÅ¨Í∏∞Î•º ÎßûÏ∂∞Ï§å\n",
        "ensemble_preds = [np.array(preds) for preds in ensemble_preds]\n",
        "min_length = min([len(preds) for preds in ensemble_preds])\n",
        "ensemble_preds = [preds[:min_length] for preds in ensemble_preds]\n",
        "\n",
        "# ÏïôÏÉÅÎ∏î Í≤∞Í≥º Í≥ÑÏÇ∞\n",
        "final_ensemble_preds = np.mean(ensemble_preds, axis=0)\n",
        "\n",
        "# Ïò¨Î∞îÎ•¥Í≤å Ïù∏Îç±Ïä§Î•º ÏÇ¨Ïö©ÌïòÏó¨ RMSE Í≥ÑÏÇ∞\n",
        "val_index = np.concatenate([val_index for _, val_index in kf.split(chembl_data)])\n",
        "val_rmse = np.sqrt(mean_squared_error(chembl_data.iloc[val_index[:min_length]]['pIC50'], final_ensemble_preds))\n",
        "print(f'Ensemble Validation RMSE: {val_rmse}')\n",
        "\n",
        "# ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞ ÏòàÏ∏°\n",
        "test = pd.read_csv('./test.csv')\n",
        "test_dataset = SMILESDataset(test, tokenizer, has_target=False)\n",
        "\n",
        "ensemble_test_preds = []\n",
        "\n",
        "for fold_idx in range(CFG['K_FOLDS']):\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(f'./results_fold_{fold_idx}', num_labels=1)\n",
        "    trainer = Trainer(model=model)\n",
        "    test_preds = trainer.predict(test_dataset)\n",
        "    ensemble_test_preds.append(test_preds.predictions.flatten())\n",
        "\n",
        "# ÏµúÏ¢Ö ÏïôÏÉÅÎ∏î ÌÖåÏä§Ìä∏ ÏòàÏ∏°\n",
        "final_test_preds = np.mean(ensemble_test_preds, axis=0)\n",
        "\n",
        "# pIC50ÏùÑ IC50ÏúºÎ°ú Î≥ÄÌôò\n",
        "def pIC50_to_IC50(pic50_values):\n",
        "    return 10 ** (9 - pic50_values)\n",
        "\n",
        "test['IC50_nM'] = pIC50_to_IC50(final_test_preds)\n",
        "\n",
        "# Ï†úÏ∂ú ÌååÏùº Ï†ÄÏû•\n",
        "submit = pd.read_csv('./sample_submission.csv')\n",
        "submit['IC50_nM'] = test['IC50_nM']\n",
        "submit.to_csv('./transformer_ensemble_submit.csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rJ2JmPBXOaKa",
        "outputId": "7e4f0d1e-44e3-4096-e983-09947606e636"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training fold 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at seyonec/PubChem10M_SMILES_BPE_450k and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='980' max='980' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [980/980 01:41, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Rmse</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>31.124796</td>\n",
              "      <td>5.578960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.220578</td>\n",
              "      <td>1.104797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.844613</td>\n",
              "      <td>0.919029</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.696253</td>\n",
              "      <td>0.834418</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.558804</td>\n",
              "      <td>0.747532</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>13.039800</td>\n",
              "      <td>0.887116</td>\n",
              "      <td>0.941868</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>13.039800</td>\n",
              "      <td>0.633695</td>\n",
              "      <td>0.796049</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>13.039800</td>\n",
              "      <td>0.548072</td>\n",
              "      <td>0.740319</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>13.039800</td>\n",
              "      <td>0.690251</td>\n",
              "      <td>0.830814</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>13.039800</td>\n",
              "      <td>0.739088</td>\n",
              "      <td>0.859702</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training fold 2/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at seyonec/PubChem10M_SMILES_BPE_450k and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='980' max='980' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [980/980 01:43, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Rmse</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>29.958324</td>\n",
              "      <td>5.473420</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.061877</td>\n",
              "      <td>1.030474</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.830433</td>\n",
              "      <td>0.911281</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.718589</td>\n",
              "      <td>0.847696</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.738732</td>\n",
              "      <td>0.859495</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>12.529500</td>\n",
              "      <td>0.832161</td>\n",
              "      <td>0.912229</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>12.529500</td>\n",
              "      <td>0.595247</td>\n",
              "      <td>0.771523</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>12.529500</td>\n",
              "      <td>0.680827</td>\n",
              "      <td>0.825123</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>12.529500</td>\n",
              "      <td>0.580379</td>\n",
              "      <td>0.761826</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>12.529500</td>\n",
              "      <td>0.648351</td>\n",
              "      <td>0.805203</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training fold 3/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at seyonec/PubChem10M_SMILES_BPE_450k and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='980' max='980' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [980/980 01:42, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Rmse</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>31.337585</td>\n",
              "      <td>5.597998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.089720</td>\n",
              "      <td>1.043897</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.575194</td>\n",
              "      <td>0.758415</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.831460</td>\n",
              "      <td>0.911844</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.650155</td>\n",
              "      <td>0.806322</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>12.419100</td>\n",
              "      <td>0.860149</td>\n",
              "      <td>0.927442</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>12.419100</td>\n",
              "      <td>0.618002</td>\n",
              "      <td>0.786131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>12.419100</td>\n",
              "      <td>0.613324</td>\n",
              "      <td>0.783150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>12.419100</td>\n",
              "      <td>0.689739</td>\n",
              "      <td>0.830505</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>12.419100</td>\n",
              "      <td>0.566260</td>\n",
              "      <td>0.752503</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training fold 4/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at seyonec/PubChem10M_SMILES_BPE_450k and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='980' max='980' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [980/980 01:44, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Rmse</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>30.598385</td>\n",
              "      <td>5.531581</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.953274</td>\n",
              "      <td>0.976358</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.544108</td>\n",
              "      <td>0.737637</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.847588</td>\n",
              "      <td>0.920645</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.591387</td>\n",
              "      <td>0.769017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>12.488600</td>\n",
              "      <td>0.851984</td>\n",
              "      <td>0.923030</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>12.488600</td>\n",
              "      <td>0.652355</td>\n",
              "      <td>0.807685</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>12.488600</td>\n",
              "      <td>0.662631</td>\n",
              "      <td>0.814021</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>12.488600</td>\n",
              "      <td>0.593564</td>\n",
              "      <td>0.770431</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>12.488600</td>\n",
              "      <td>0.593447</td>\n",
              "      <td>0.770355</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training fold 5/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at seyonec/PubChem10M_SMILES_BPE_450k and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='980' max='980' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [980/980 01:41, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Rmse</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>29.266417</td>\n",
              "      <td>5.409844</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.034485</td>\n",
              "      <td>1.017096</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.736941</td>\n",
              "      <td>0.858453</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.788320</td>\n",
              "      <td>0.887874</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.763934</td>\n",
              "      <td>0.874033</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>12.568500</td>\n",
              "      <td>0.769360</td>\n",
              "      <td>0.877132</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>12.568500</td>\n",
              "      <td>0.813334</td>\n",
              "      <td>0.901850</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>12.568500</td>\n",
              "      <td>0.856134</td>\n",
              "      <td>0.925275</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>12.568500</td>\n",
              "      <td>0.720677</td>\n",
              "      <td>0.848927</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>12.568500</td>\n",
              "      <td>0.797771</td>\n",
              "      <td>0.893180</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ensemble Validation RMSE: 0.5112974297320854\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fHtki24pOaNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iMYPQXQQOaQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LHoWXzk7OaUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aLNz2g90OaXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bWxqhf5pOabN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wnIR7XFIOaew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ztAnJRhjOaiY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyNd13xOW9EXQmo0ma6+/e4Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}